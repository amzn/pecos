{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e70878",
   "metadata": {},
   "source": [
    "# How to Apply PECOS to Extreme Multi-label Classification\n",
    "\n",
    "Prediction for Enormous and Correlated Output Spaces (PECOS) is a versatile and modular machine learning framework for solving prediction problems with very large outputs spaces. For a given input instance, we apply PECOS to the eXtreme Multilabel Classification (XMC) problem to find and rank the most relevant items from an enormous but fixed and finite output space. Generally, PECOS trains an XMC model that takes numerical features to rank labels from the enormous output space. PECOS also provides feature extraction functions for text data, such as TF-IDF (this session) and Transformers (Session 5).\n",
    "\n",
    "<div><br/>\n",
    "<img src=\"imgs/pecos_pipeline.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Using PECOS, we can tackle lots of real-world large-scale applications with only few commands or limited programming codes.\n",
    "\n",
    "<div><br/>\n",
    "<img src=\"imgs/pecos_xmc_examples.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this part of the tutorial, we will use XR-Linear as an example to demonstrate how to use PECOS to tackle real-world problems and understrand the model architecture in PECOS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c281dc",
   "metadata": {},
   "source": [
    "## Outline in this Session\n",
    "\n",
    "1. Experimental dataset preparation\n",
    "2. Hands-on PECOS in only few commands \n",
    "3. Code with the PECOS library\n",
    "4. Build your customized PECOS XR-Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d87d24",
   "metadata": {},
   "source": [
    "## 1. Experimental Dataset Preparation\n",
    "\n",
    "`eurlex-4k`, `wiki10-31k`, `amazoncat-13k`, `amazon-670k`, `wiki-500k`, and `amazon-3m` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1073ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmc-base/wiki10-31k/output-items.txt\r\n",
      "xmc-base/wiki10-31k/tfidf-attnxml\r\n",
      "xmc-base/wiki10-31k/tfidf-attnxml/X.trn.npz\r\n",
      "xmc-base/wiki10-31k/tfidf-attnxml/X.tst.npz\r\n",
      "xmc-base/wiki10-31k/X.trn.txt\r\n",
      "xmc-base/wiki10-31k/X.tst.txt\r\n",
      "xmc-base/wiki10-31k/Y.trn.npz\r\n",
      "xmc-base/wiki10-31k/Y.trn.txt\r\n",
      "xmc-base/wiki10-31k/Y.tst.npz\r\n",
      "xmc-base/wiki10-31k/Y.tst.txt\r\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"wiki10-31k\"\n",
    "! wget -nv -nc https://archive.org/download/pecos-dataset/xmc-base/{DATASET}.tar.gz\n",
    "! tar --skip-old-files -zxf {DATASET}.tar.gz \n",
    "! find xmc-base/{DATASET}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fb642",
   "metadata": {},
   "source": [
    "### Numerical Feature and Label Format in PECOS\n",
    "\n",
    "In PECOS, numerical features of instances can be in either a [dense NumPy matrix](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) or a [Compressed Sparse Row (CSR) matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) of shape `(nr_inst, nr_feat)`, where `nr_inst` and `nr_feat` are numbers of instances and features. \n",
    "\n",
    "Similary, labels of instances can be also presented as a dense or a sparse matrix of shape `(nr_inst, nr_labels)`, where `nr_labels` is the number of labels in the XMC problem. The following figure shows an example of the sparse matrix for label representations of six instances.\n",
    "\n",
    "<div><br/>\n",
    "<img src=\"imgs/pecos_label_matrix.png\" width=\"80%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8d41d",
   "metadata": {},
   "source": [
    "## 2. Hands-on PECOS in Only Few Commands\n",
    "\n",
    "PECOS provides convenient command-line interfaces to establish a pipeline from feature extraction to training and inference. Specifically, a PECOS XR-Linear model for text data can be established and evaluated using the following command-line modules without writing any code.\n",
    "\n",
    "* Text Vectorizer: `pecos.utils.featurization.text.preprocess`\n",
    "* XR-Linear Train/Predict/Evaluate: `pecos.xmc.xlinear.train`, `pecos.xmc.xlinear.predict`, `pecos.xmc.xlinear.evaluate`\n",
    "\n",
    "All of these commands can be supplied with JSON-format configuration files (See Section 4.2.2 and Appendix 2). In this section, we first use the default setting to have a quick hands-on demo.\n",
    "\n",
    "\n",
    "### 2.1. Text Vectorizer\n",
    "\n",
    "PECOS text vectorizer `pecos.utils.featurization.text.preprocess` extracts TF-IDF features. The options `build` and `run` learn the vectorizer and extract features, respectively. The `--help` argument will list all available command-line options. In the default setting, the vectorizer learns a unigram TF-IDF without any filtering. If you have prepared a JSON-format vectorizer configuration file, the argument `--vectorizer-config-path` can help customize the vectorizer (See Section 4.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b47e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pecos.utils.featurization.text.preprocess build \\\n",
    "        --text-pos 0 \\\n",
    "        --input-text-path xmc-base/{DATASET}/X.trn.txt \\\n",
    "        --output-model-folder simplest.{DATASET}.vectorizer \\\n",
    "        --from-file true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870f6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
    "            --text-pos 0 \\\n",
    "            --input-preprocessor-folder simplest.{DATASET}.vectorizer \\\n",
    "            --input-text-path xmc-base/{DATASET}/X.trn.txt \\\n",
    "            --output-inst-path simplest.{DATASET}.X.trn.npz \\\n",
    "            --from-file true\n",
    "! python3 -m pecos.utils.featurization.text.preprocess run \\\n",
    "            --text-pos 0 \\\n",
    "            --input-preprocessor-folder simplest.{DATASET}.vectorizer \\\n",
    "            --input-text-path xmc-base/{DATASET}/X.tst.txt \\\n",
    "            --output-inst-path simplest.{DATASET}.X.tst.npz \\\n",
    "            --from-file true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bb4e2",
   "metadata": {},
   "source": [
    "### 2.2. Train, Predict, Evaluate a PECOS Model\n",
    "\n",
    "With feature and label matrices, the pipeline of training, prediction, and evalution can be easily established with the modules `pecos.xmc.xlinear.train`, `pecos.xmc.xlinear.predict`, and `pecos.xmc.xlinear.evaluate`. The `--help` argument will list all available command-line options. If you have prepared a JSON-format configuration file, the argument `--params-path` can help customize training and prediction procedures (See Appendix 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2316bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pecos.xmc.xlinear.train \\\n",
    "    -x simplest.{DATASET}.X.trn.npz \\\n",
    "    -y xmc-base/{DATASET}/Y.trn.npz \\\n",
    "    -m simplest.{DATASET}.model \\\n",
    "    --nr-splits 16 \\\n",
    "    -t 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580ecfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== evaluation results ====\r\n",
      "prec   = 83.89 78.41 72.53 67.68 63.45 59.72 56.48 53.53 50.86 48.39\r\n",
      "recall = 4.95 9.21 12.64 15.60 18.11 20.34 22.30 24.07 25.63 27.00\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pecos.xmc.xlinear.predict \\\n",
    "    -x simplest.{DATASET}.X.tst.npz \\\n",
    "    -y xmc-base/{DATASET}/Y.tst.npz \\\n",
    "    -m simplest.{DATASET}.model \\\n",
    "    -o simplest.{DATASET}.Y.tst.pred.npz \\\n",
    "    -b 10 \\\n",
    "    -k 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e75b408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== evaluation results ====\r\n",
      "prec   = 83.89 78.41 72.53 67.68 63.45 59.72 56.48 53.53 50.86 48.39\r\n",
      "recall = 4.95 9.21 12.64 15.60 18.11 20.34 22.30 24.07 25.63 27.00\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pecos.xmc.xlinear.evaluate \\\n",
    "    -y xmc-base/{DATASET}/Y.tst.npz \\\n",
    "    -p simplest.{DATASET}.Y.tst.pred.npz \\\n",
    "    -k 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c731f5",
   "metadata": {},
   "source": [
    "## 3. Code with the PECOS library\n",
    "\n",
    "PECOS includes the comprehensieve Python library and interfaces so that we can easily utilize PECOS in the code-level with more flexibility.\n",
    "\n",
    "### 3.1. Loading Features and Labels\n",
    "For convenience, PECOS also provides APIs `load_feature_matrix` and `load_label_matrix` for loading features and labels from binary files in arbitary formats.\n",
    "Note that for the sparse format, training labels should be loaded as a [Compressed Sparse Column (CSC) matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) while testing labels should be  loaded as a CSR matrix for the purpose of computational efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c518d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features X_trn is a csr matrix of shape (14146, 101938).\n",
      "Training labels Y_trn is a csc matrix of shape (14146, 30938).\n",
      "Testing features X_tst is a csr matrix of shape (6616, 101938).\n",
      "Testing labels Y_tst is a csr matrix of shape (6616, 30938).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pecos.xmc.xlinear.model import XLinearModel\n",
    "\n",
    "DATASET = \"wiki10-31k\"\n",
    "\n",
    "X_trn = XLinearModel.load_feature_matrix(f\"xmc-base/{DATASET}/tfidf-attnxml/X.trn.npz\")\n",
    "Y_trn = XLinearModel.load_label_matrix(f\"xmc-base/{DATASET}/Y.trn.npz\", for_training=True)\n",
    "\n",
    "X_tst = XLinearModel.load_feature_matrix(f\"xmc-base/{DATASET}/tfidf-attnxml/X.tst.npz\")\n",
    "Y_tst = XLinearModel.load_label_matrix(f\"xmc-base/{DATASET}/Y.tst.npz\", for_training=False)\n",
    "\n",
    "print(f\"Training features X_trn is a {X_trn.getformat()} matrix of shape {X_trn.shape}.\")\n",
    "print(f\"Training labels Y_trn is a {Y_trn.getformat()} matrix of shape {Y_trn.shape}.\")\n",
    "print(f\"Testing features X_tst is a {X_tst.getformat()} matrix of shape {X_tst.shape}.\")\n",
    "print(f\"Testing labels Y_tst is a {Y_tst.getformat()} matrix of shape {Y_tst.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fea14",
   "metadata": {},
   "source": [
    "### 3.2. Semantic Label Indexing and Cluster Chain in XR-Linear\n",
    "\n",
    "The first step of training an XR-Linear model is to conduct semantic label indexing and establish the *hierarchial label tree* for resursive training the XR-Linear model and its inference. \n",
    "\n",
    "<div><br/>\n",
    "<img src=\"imgs/pecos_matcher_ranker.png\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "PECOS supports any method for semantic label indexing. In the PECOS library, as a build-in method, we provide Label Representation via Positive Instance Feature Aggregation (PIFA) for semantic label indexing with only the need of positive instances and their features in training data. PECOS can also consider additional label features `Z` of shape `(nr_labels, nr_label_feat)` in either dense or sparse matrix format, where `nr_label_feat` is the number of label features. These representations and features for each label are concatenated or combined as label embedding in `LabelEmbeddingFactory` in PECOS.\n",
    "\n",
    "To conduct semantic label indexing, PECOS learns an indexer based on label embedding. PECOS currently supports to use the **Hierarchical K-Means** for semantic label indexing with a hyper-parameter `nr_splits` (the number of clusters in each layer, or `B` in [our report](https://arxiv.org/pdf/2010.05878.pdf)), which decides the depth `D` of the hierarchical label tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26794215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layers in the trained hierarchical label tree.\n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "\n",
    "label_feat = LabelEmbeddingFactory.create(Y_trn, X_trn, method=\"pifa\")\n",
    "# label_feat = LabelEmbeddingFactory.create(Y_trn, X_trn, Z, method=\"pifa_lf_concat\") # for using label features Z\n",
    "\n",
    "cluster_chain = Indexer.gen(label_feat, nr_splits=16, indexer_type=\"hierarchicalkmeans\")\n",
    "\n",
    "print(f\"{len(cluster_chain)} layers in the trained hierarchical label tree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffda21",
   "metadata": {},
   "source": [
    "### 3.3. Training XR-Linear Negative Sampling and Sparsification\n",
    "\n",
    "Negative sampling plays an important role in solving the XMC problem. PECOS currently provides two negative sampling schemes, including Teacher Forcing Negatives (TFN) and Matcher Aware Negatives (MAN). Please refer to [our report](https://arxiv.org/pdf/2010.05878.pdf) for more details about negative sampling schemes.\n",
    "\n",
    "To reduce model sizes and improve efficiency, PECOS conduct model sparsification with a hyper-parameter `threshold`. The model weights with absolute values smaller than the threshold will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3d6527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 39.7176 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# For negative_sampling_scheme in model training, \"man\" and tfn+man\" are also available.\n",
    "xlm = XLinearModel.train(X_trn, Y_trn, C=cluster_chain, threshold=0.1, negative_sampling_scheme=\"tfn\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training time: {training_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5cfa7",
   "metadata": {},
   "source": [
    "PECOS supports serializing and loading the trained model into binary on disk with convenient interfaces. Note that model loading with `is_predict_only=True` could lead to faster prediction speed by disabling the flexibility of model modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5d468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm.save(f\"{DATASET}.xlm.model\")\n",
    "xlm = XLinearModel.load(f\"{DATASET}.xlm.model\", is_predict_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6038ec",
   "metadata": {},
   "source": [
    "### 3.4. Prediction and Evaluation\n",
    "\n",
    "As a tree model, the inference method significantly affects the prediction efficiency of XR-Linear in PECOS. As illustrated in the following figure, the prediction process in PECOS employs a beam search with a hyper-parameter `beam_size`. The other hyper-parameter `only_topk` also needs to be decided to limit the predicted most relevant labels for each instance. The `predict` function of the trained model will result in a CSR matrix of shape `(nr_inst, nr_labels)` and exactly `only_topk` non-zero columns for each row (or instance).\n",
    "\n",
    "<div>\n",
    "<br/><img src=\"imgs/pecos_beam_search.png\" width=\"50%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f851bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred is a csr matrix of shape (6616, 30938) and 66160 non-zero elements.\n"
     ]
    }
   ],
   "source": [
    "Y_pred = xlm.predict(X_tst, beam_size=10, only_topk=10)\n",
    "\n",
    "print(f\"Y_pred is a {Y_pred.getformat()} matrix of shape {Y_pred.shape} and {Y_pred.nnz} non-zero elements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ed22c",
   "metadata": {},
   "source": [
    "For evaluation, we evaluate the trained model with conventional ranking metrics, including Precision@K and Recall@K. PECOS also provides the evaluation interface for predicted sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c57da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 84.36 78.20 72.67 68.03 63.85 60.23 56.86 53.85 51.07 48.63\n",
      "recall = 4.99 9.17 12.65 15.67 18.28 20.56 22.50 24.21 25.74 27.15\n"
     ]
    }
   ],
   "source": [
    "from pecos.utils import smat_util\n",
    "metrics = smat_util.Metrics.generate(Y_tst, Y_pred, topk=10)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9baf18",
   "metadata": {},
   "source": [
    "### 3.5. PECOS and One-versus-All (OVA) Model\n",
    "\n",
    "PECOS also supports to train an OVA model without leveraing clustering hierarchy if needed. \n",
    "\n",
    "**Training OVA models is time-consuming, we suggest to try the following code offline after the tutorial. Note that training the above XR-Linear model is 26 times faster than training an OVA model using an AWS *i3.4xlarge* instance.**\n",
    "\n",
    "```python\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "xlm_ova = XLinearModel.train(X_trn, Y_trn, C=None, negative_sampling_scheme=\"tfn\") \n",
    "\n",
    "training_time_ova = time.time() - start_time\n",
    "print(f\"Training time for the OVA model: {training_time_ova:.4f} seconds.\")\n",
    "pecos_faster_ratio = training_time_ova / training_time\n",
    "print(f\"XR-Linear is {pecos_faster_ratio:.2f} times faster than the OVA model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663277d",
   "metadata": {},
   "source": [
    "# 4. Customized PECOS Model\n",
    "\n",
    "Besides pre-defined models in PECOS, such as XR-Linear, it is also convenient for users to customize PECOS for specific purposes and usage. Specifically, we suggest to establishing a model class to wrap fundamental PECOS functions and tailored operations. As a result, the customized model can be easily constructed and consumed for arbitrary data types and feature extractors. \n",
    "\n",
    "## 4.1. Structure of a Customized PECOS Model\n",
    "\n",
    "Even though a customized machine learning pipeline can be seperated into several independent scripts, we recommend declaring a customized PECOS model as a **model class** for better re-usability and code maintenance.\n",
    "\n",
    "A customized PECOS model should at least consist of the following components:\n",
    "\n",
    "* `preprocessor` or `encoder`: The procedure, which can be a method or a functionable object, pre-processes or encodes an arbitrary input with the designated data format into features. For example, text data and image data can be encoded by BERT and ResNet.\n",
    "* `train()`: The training method takes a set of training data with a preprocessor, learns a primitive PECOS model, and returns a PECOS-based customized machine learning model. The training function could be a class method to construct the model object with the learned model and essential components after training.\n",
    "* `model`: A primitive PECOS model taking pre-processed features is capable of deriving the predictions for arbitrary testing data. The model weights should be learned by `train()`. \n",
    "* `predict()`: The prediction method takes arbitrary testing data and infers the prediction based on the pre-processor and the learned model.\n",
    "* `save()`: The saving function serializes the trained model, including model weights and configuration, for further usage.\n",
    "* `load()`: The loading function reads the serialized model so that the trained model can be loaded and re-used.\n",
    "\n",
    "<div>\n",
    "<br/><img src=\"imgs/illus_customized_model.jpg\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this part of the tutorial, we will use the task of *extreme multi-label text classification* as an example to demonstrate how to **customize a PECOS model that can handle text data with either a conventional bag-of-words (BoW) model or a deep learning model as the text encoder for feature extraction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acc325",
   "metadata": {},
   "source": [
    "## 4.2. Example: eXtreme Multi-label Text Classification (XMTC)\n",
    "\n",
    "The task of extreme multi-label text classification (XMTC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMTC tasks, such as recommendation systems, document tagging, and semantic search. \n",
    "\n",
    "In this section, we guide through how to establish a customized PECOS model for XMTC tasks. We will walk through (1) PECOS' built-in BOW model for text preprocessing and vectorizing; (2) how to customize a PECOS model; and (3) \n",
    "advanced usage of XR-Transformer based on deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237164ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text2text_demo/output-labels.txt\r\n",
      "text2text_demo/testing-data.txt\r\n",
      "text2text_demo/training-data.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wget -nv -nc https://archive.org/download/text2text_demo.tar.gz/text2text_demo.tar.gz\n",
    "! tar --skip-old-files -zxf text2text_demo.tar.gz\n",
    "! find text2text_demo/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ec0e6",
   "metadata": {},
   "source": [
    "### 4.2.1. Preprocessor: Text Preprocessing and Vectorizing\n",
    "\n",
    "The preprocessor plays a role of encoding input data into machine readable vector representations. Any encoder that can transform text data into a vector representation can be considered as the preprocessor or encoder of a customized PECOS model for XMTC tasks.\n",
    "\n",
    "In the PECOS library, we provide [various text vectorizers](https://github.com/amzn/pecos/blob/mainline/pecos/utils/featurization/text/vectorizers.py), such as TF-IDF, hashing, and pretrained transformer, as **built-in preprocessors** to deal with text data. In this tutorial, we will utilize the [n-gram](https://en.wikipedia.org/wiki/N-gram)  [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) model as our preprocessor.\n",
    "\n",
    "#### Label Space File Format  for Built-in Text Preprocessors\n",
    "\n",
    "Label space is also essential for text preprocessors, especially for understanding the label space size to create the appropriate label matrix. The label IDs start from zero and can be referred to the line numbers and corresponding text descriptions in the label space file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f48f4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence researchers\r\n",
      "Computability theorists\r\n",
      "British computer scientists\r\n",
      "Machine learning researchers\r\n",
      "Turing Award laureates\r\n",
      "Deep Learning\r\n"
     ]
    }
   ],
   "source": [
    "! cat \"./text2text_demo/output-labels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0862645",
   "metadata": {},
   "source": [
    "#### Data File Format  for Built-in Text Preprocessors\n",
    "\n",
    "PECOS built-in text preprocessors majorly take the files of text data with labels in a tab-separated values (TSV) format. Each line in the TSV file consists of two elements that represent the comma-separated label IDs and the input text of a data instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5ebfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\tAlan Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\r\n",
      "0,2,3\tHinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks.\r\n",
      "3,4,5\tHinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on artificial intelligence and deep learning.\r\n",
      "0,3,5\tYoshua Bengio is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning.\r\n"
     ]
    }
   ],
   "source": [
    "! cat ./text2text_demo/training-data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d1eb5",
   "metadata": {},
   "source": [
    "The data file format also supports to represent the label relevance for cost-sensitive learning by using double colons to separate a label and its relevance.\n",
    "\n",
    "<p style=\"text-align: center;\"><i>\n",
    "0::0.1,1::0.2,2::0.8 &lt;TAB&gt; Alan Turing is widely considered to be the father of theoretical computer science and artificial intelligence.</i></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e4419",
   "metadata": {},
   "source": [
    "#### Training a Text Preprocessor\n",
    "\n",
    "The preprocessor model `Preprocessor` is defined in `pecos.utils.featurization.text.preprocess`. Given a training text corpus and the configuration dictionary, the class method `Preprocessor.train` will train a corresponding text preprocesssor. Besides, the built-in preprocessors also support serialization with the function `save()` for the re-usability.\n",
    "\n",
    "With the previously mentioned data and label space file formats, the utility function `Preprocessor.load_data_from_file(input_text_path, output_text_path)` returns a dictionary with three keys:\n",
    "\n",
    "* `label_matrix`: a `(num_inst, num_labels)` CSR matrix for the labels of each instance.\n",
    "* `label_relevance`: `None` or a `(num_inst, num_labels)` CSR matrix for the relevance of each label in cost-sensitive learning if available.\n",
    "* `corpus`: a list of string as the text corpus in the input_text_path.\n",
    "\n",
    "The configuration settings of text preprocessor including the preprocessor type and hyper-parameters should be defined in a dictionary. Specifially, the key `type` defines the preprocessor choice while the key `kwargs` represents the hyper-parameters. In this tutorial, we adopt n-gram TFIDF features containing *word unigrams*, *word bigrams*, and *character trigrams*. Note that each of the n-gram feature can have different hyper-parameters, such as `max_feature` and `max_df`. Users need to properly set max_feature (e.g., hundred of thousands or millions) based on the corpus size and downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f70a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pecos.utils.featurization.text.preprocess import Preprocessor\n",
    "\n",
    "input_text_path = \"./text2text_demo/training-data.txt\"\n",
    "output_text_path = \"./text2text_demo/output-labels.txt\"\n",
    "model_folder = \"./text2text_demo/pecos-text2text-model\"\n",
    "\n",
    "parsed_result = Preprocessor.load_data_from_file(input_text_path, output_text_path) # Read files\n",
    "corpus = parsed_result[\"corpus\"] # Corpus input text: List of strings\n",
    "Y = parsed_result[\"label_matrix\"] # Label Matrix: Sparse Matrix\n",
    "\n",
    "vectorizer_config = {\n",
    "    \"type\": \"tfidf\",\n",
    "    \"kwargs\": {\n",
    "      \"base_vect_configs\": [\n",
    "          \n",
    "        {\n",
    "          \"ngram_range\": [1, 1],\n",
    "          \"max_df_ratio\": 0.98,\n",
    "          \"analyzer\": \"word\",\n",
    "        },\n",
    "        {\n",
    "          \"ngram_range\": [2, 2],\n",
    "          \"max_df_ratio\": 0.98,\n",
    "          \"analyzer\": \"word\",\n",
    "        },\n",
    "        {\n",
    "          \"ngram_range\": [3, 3],\n",
    "          \"max_df_ratio\": 0.98,\n",
    "          \"analyzer\": \"char_wb\",\n",
    "        },\n",
    "      ],\n",
    "    },\n",
    "  }\n",
    "\n",
    "preprocessor = Preprocessor.train(corpus, vectorizer_config)\n",
    "preprocessor.save(model_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0300f8c",
   "metadata": {},
   "source": [
    "#### Preprocessing with a Trained Text Preprocessor\n",
    "\n",
    "The function `predict` of a trained text preprocessor encodes texts in a **text data file** into a CSR matrix of shape `(num_inst, dim)` as numerical vector representations, where `num_inst` is the number of instances in the file; `dim` is the number of feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b182171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file consists of 4 instances with {X.shape[1]}-dimensional features in a {X.getformat()} matrix.\n",
      "\n",
      "Text 0: Alan Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\n",
      "Text 1: Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks.\n",
      "Text 2: Hinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on artificial intelligence and deep learning.\n",
      "Text 3: Yoshua Bengio is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning.\n",
      "\n",
      "The cosine similarity is 0.0076 between text 0 and text 1.\n",
      "The cosine similarity is 0.0325 between text 0 and text 2.\n",
      "The cosine similarity is 0.0082 between text 1 and text 2.\n",
      "The cosine similarity is 0.0366 between text 0 and text 3.\n",
      "The cosine similarity is 0.0267 between text 1 and text 3.\n",
      "The cosine similarity is 0.0943 between text 2 and text 3.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining numerical vectors from text\n",
    "X = preprocessor.predict(corpus)\n",
    "\n",
    "print(f\"The file consists of {X.shape[0]} instances \"\n",
    "      \"with {X.shape[1]}-dimensional features \"\n",
    "      \"in a {X.getformat()} matrix.\\n\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(X)\n",
    "\n",
    "for i, ti in enumerate(corpus):\n",
    "    print(f\"Text {i}: {ti}\")\n",
    "\n",
    "print(\"\")\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(i):\n",
    "        print(f\"The cosine similarity is {sim[i][j]:.4f} between text {j} and text {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcd09b",
   "metadata": {},
   "source": [
    "#### Command-line Interface\n",
    "\n",
    "The above vectorizer operations can also be achieved by the following commands with a JSON-format configuration file:\n",
    "\n",
    "```bash\n",
    "python3 -m pecos.utils.featurization.text.preprocess build \\\n",
    "    --text-pos 1 \\\n",
    "    --input-text-path ./text2text_demo/training-data.txt \\\n",
    "    --vectorizer-config-path /path/to/vectorizer-config.json \\\n",
    "    --output-model-folder ./text2text_demo/pecos-text2text-model\n",
    "\n",
    "python3 -m pecos.utils.featurization.text.preprocess run \\\n",
    "    --input-preprocessor-folder ./text2text_demo/pecos-text2text-model \\\n",
    "    --text-pos 1 \\\n",
    "    --input-text-path ./text2text_demo/training-data.txt \\\n",
    "    --output-inst-path /path/to/X.npz\n",
    "    --label-pos 0 \\\n",
    "    --output-label-path /path/to/Y.npz \\\n",
    "    --label-text-path ./text2text_demo/output-labels.txt\n",
    "```\n",
    "\n",
    "#### Efficiency of PECOS Built-in TF-IDF Vectorizer\n",
    "\n",
    "Moreover, the TF-IDF vectorizer in PECOS is implemented in C++ and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3d6f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PECOS TFIDF time: 27.13237s, result shape=(14146, 10858825), nnz=37194670\n"
     ]
    }
   ],
   "source": [
    "vectorizer_config = {\n",
    "    \"type\": \"tfidf\",\n",
    "    \"kwargs\": {\n",
    "      \"base_vect_configs\": [ \n",
    "        {\n",
    "          \"ngram_range\": [1, 2],\n",
    "          \"max_df_ratio\": 0.98,\n",
    "          \"analyzer\": \"word\",\n",
    "        },\n",
    "      ],\n",
    "    },\n",
    "  }\n",
    "\n",
    "input_text_path = \"xmc-base/wiki10-31k/X.trn.txt\"\n",
    "corpus = Preprocessor.load_data_from_file(input_text_path, text_pos=0)[\"corpus\"]\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "preprocessor = Preprocessor.train(corpus, vectorizer_config)\n",
    "X = preprocessor.predict(input_text_path)\n",
    "print(f\"PECOS TFIDF time: {time.time() - start_time:.5f}s, result shape={X.shape}, nnz={X.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c62ce",
   "metadata": {},
   "source": [
    "As a baseline method, we compare with the [Sklearn TFIDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):\n",
    "```python\n",
    "start_time = time.time()\n",
    "preprocessor = Preprocessor.train(\n",
    "    corpus,\n",
    "    {\"type\": \"sklearntfidf\", \"kwargs\":{\"ngram_range\": [1, 2], \"max_df\": 0.98}},\n",
    ")\n",
    "X = preprocessor.predict(corpus)\n",
    "print(f\"Sklearn TFIDF time: {time.time() - start_time:.5f}s, result shaepe={X.shape}, nnz={X.nnz}\")\n",
    "```\n",
    "\n",
    "**Training Sklearn TFIDF models is time-consuming, we suggest to try the following code offline after the tutorial. The execution results using an AWS *i3.4xlarge* instance are as follows:**\n",
    "```\n",
    "Sklearn TFIDF time: 221.40709s, result shaepe=(14146, 7269690), nnz=33505461\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5aaf8",
   "metadata": {},
   "source": [
    "### 4.2.2 Customized PECOS Model with TF-IDF Preprocessor\n",
    "\n",
    "\n",
    "After being powered with text preprocessors, following the [aforementioned illustration](#Structure-of-a-Customized-PECOS-Model), we demonstrate an example of declaring a **customized PECOS model class** based on a TF-IDF preprocessor and a XR-Linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3893c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "import pathlib\n",
    "from pecos.utils.featurization.text.preprocess import Preprocessor\n",
    "from pecos.xmc.xlinear.model import XLinearModel\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "from pecos.utils import smat_util\n",
    "\n",
    "class CustomPECOS:\n",
    "    def __init__(self, preprocessor=None, xlinear_model=None, output_items=None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.xlinear_model = xlinear_model\n",
    "        self.output_items = output_items\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, input_text_path, output_text_path):\n",
    "        \"\"\"Train a CustomPECOS model\n",
    "        \n",
    "        Args: \n",
    "            input_text_path (str): Text input file name.            \n",
    "            output_text_path (str): The file path for output text items.\n",
    "            vectorizer_config (str): Json_format string for vectorizer config (default None). e.g. {\"type\": \"tfidf\", \"kwargs\": {}}\n",
    "            \n",
    "        Returns:\n",
    "            A CustomPECOS object\n",
    "        \"\"\"\n",
    "        # Obtain X_text, Y\n",
    "        parsed_result = Preprocessor.load_data_from_file(input_text_path, output_text_path)\n",
    "        Y = parsed_result[\"label_matrix\"]\n",
    "        corpus = parsed_result[\"corpus\"]\n",
    "\n",
    "        # Train TF-IDF vectorizer\n",
    "        preprocessor = Preprocessor.train(corpus, {\"type\": \"tfidf\", \"kwargs\":{}}) \n",
    "        X = preprocessor.predict(corpus)   \n",
    "        \n",
    "        # Train a XR-Linear model with TF-IDF features\n",
    "        label_feat = LabelEmbeddingFactory.create(Y, X, method=\"pifa\")\n",
    "        cluster_chain = Indexer.gen(label_feat)\n",
    "        xlinear_model = XLinearModel.train(X, Y, C=cluster_chain)\n",
    "        \n",
    "        # Load output items\n",
    "        with open(output_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            output_items = [q.strip() for q in f]\n",
    "        \n",
    "        return cls(preprocessor, xlinear_model, output_items)\n",
    "    \n",
    "    def predict(self, corpus):\n",
    "        \"\"\"Predict labels for given inputs\n",
    "        \n",
    "        Args:\n",
    "            corpus (list of strings): input strings.\n",
    "        Returns:\n",
    "            csr_matrix: predicted label matrix (num_samples x num_labels)\n",
    "        \"\"\"\n",
    "        X = self.preprocessor.predict(corpus)\n",
    "        Y_pred = self.xlinear_model.predict(X)\n",
    "        return smat_util.sorted_csr(Y_pred)\n",
    "\n",
    "    def save(self, model_folder):\n",
    "        \"\"\"Save the CustomPECOS model\n",
    "\n",
    "        Args:\n",
    "            model_folder (str): folder name to save\n",
    "        \"\"\"\n",
    "        self.preprocessor.save(f\"{model_folder}/preprocessor\")\n",
    "        self.xlinear_model.save(f\"{model_folder}/xlinear_model\")\n",
    "        with open(f\"{model_folder}/output_items.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(self.output_items, fp)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_folder):\n",
    "        \"\"\"Load the CustomPECOS model\n",
    "\n",
    "        Args:\n",
    "            model_folder (str): folder name to load\n",
    "        Returns:\n",
    "            CustomPECOS\n",
    "        \"\"\"\n",
    "        preprocessor = Preprocessor.load(f\"{model_folder}/preprocessor\")\n",
    "        xlinear_model = XLinearModel.load(f\"{model_folder}/xlinear_model\")\n",
    "        with open(f\"{model_folder}/output_items.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "            output_items = json.load(fin)\n",
    "        return cls(preprocessor, xlinear_model, output_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdbb2c6",
   "metadata": {},
   "source": [
    "### 4.2.3. Operating the Customized PECOS Model\n",
    "\n",
    "With a well-declared model class, the customized PECOS model can be modularized and very convenient to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24134357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path for model serialization and preprocessor configuration.\n",
    "model_folder = \"./text2text_demo/pecos-CustomPECOS-model\"\n",
    "\n",
    "# Train and save the trained model\n",
    "input_text_path = \"./text2text_demo/training-data.txt\"\n",
    "output_text_path = \"./text2text_demo/output-labels.txt\"\n",
    "model = CustomPECOS.train(input_text_path, output_text_path)\n",
    "model.save(model_folder)\n",
    "\n",
    "# Load the trained model and predict\n",
    "model = model.load(model_folder)\n",
    "testing_text_path = \"./text2text_demo/testing-data.txt\"\n",
    "Y_pred = model.predict(testing_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31efd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: In 1989, Yann LeCun et al. applied the standard backpropagation algorithm on neural networks for hand digit recognition.\n",
      "Score 0.9515: Machine learning researchers\n",
      "Score 0.8233: Artificial intelligence researchers\n",
      "Score 0.4659: Deep Learning\n",
      "Score 0.2779: British computer scientists\n",
      "Score 0.0569: Turing Award laureates\n",
      "Score 0.0129: Computability theorists\n"
     ]
    }
   ],
   "source": [
    "test_texts = Preprocessor.load_data_from_file(testing_text_path, output_text_path)[\"corpus\"]\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"Text Input: {text}\")\n",
    "    for j in range(Y_pred.indptr[i], Y_pred.indptr[i + 1]):\n",
    "        pred_label = model.output_items[Y_pred.indices[j]]\n",
    "        pred_score = Y_pred.data[j]\n",
    "        print(f\"Score {pred_score:.4f}: {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32908310",
   "metadata": {},
   "source": [
    "## Appedix 1: Model Parameters in PECOS Implementation\n",
    "\n",
    "###  A1.1. Cluster Chain in PECOS Implementation\n",
    "\n",
    "Specifically, PECOS trains a *cluster_chain* of `D` matching matrices `C[d]`, where `C[d]` is a CSC matrix of shape `(L[d], K[d])`; `L[d]` and `K[d]` are the numbers of labels and clusters in the layer `d`. Note that the clusters of a layer would be the labels of the next layer. The labels of the last layer `L[D - 1]` would be the labels of the overall XMC problem `nr_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b0cb55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layers in the trained hierarchical label tree with C[d] as:\n",
      "cluster_chain[0] is a csc matrix of shape (2, 1)\n",
      "cluster_chain[1] is a csc matrix of shape (32, 2)\n",
      "cluster_chain[2] is a csc matrix of shape (512, 32)\n",
      "cluster_chain[3] is a csc matrix of shape (30938, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(cluster_chain)} layers in the trained hierarchical label tree with C[d] as:\")\n",
    "for d, C in enumerate(cluster_chain):\n",
    "    print(f\"cluster_chain[{d}] is a {C.getformat()} matrix of shape {C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b21dc",
   "metadata": {},
   "source": [
    "### A2.2. Model Weights in PECOS Implementation\n",
    "\n",
    "Model weights in an XR-Linear model are also accessible as `model_chain` for analysis and computations. For the i-th layer in the hierarchy, the model weights of matchers/rankers are available as a CSC matrix of shape `(nr_feat + 1, L[i])`, which concatenates weights for features and the bias term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e101f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_chain[0].W is a csc matrix of shape (101939, 2)\n",
      "model_chain[1].W is a csc matrix of shape (101939, 32)\n",
      "model_chain[2].W is a csc matrix of shape (101939, 512)\n",
      "model_chain[3].W is a csc matrix of shape (101939, 30938)\n"
     ]
    }
   ],
   "source": [
    "for d, m in enumerate(xlm.model.model_chain):\n",
    "    print(f\"model_chain[{d}].W is a {m.W.getformat()} matrix of shape {m.W.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a599a0",
   "metadata": {},
   "source": [
    "## Appendix 2: Customized Parameters and Advanced Training Options\n",
    "\n",
    "PECOS also supports using customized parameters and several advanced training options, such as different solvers and cost-sensitive learning.\n",
    "\n",
    "### A2.1. Customized Parameters\n",
    "\n",
    "The parameters for either of indexing, training, and inference can be easily customized by feeding a dictionary into the corresponding parameter class and its constructor:\n",
    "\n",
    "* Semantic Indexing (Hierarchical K-Means): `HierarchicalKMeans.TrainParams.from_dict(dict)`\n",
    "* Training: `XLinearModel.TrainParams.from_dict(dict)`\n",
    "* Inference: `XLinearModel.PredParams.from_dict(dict)`\n",
    "\n",
    "Although most of the parameters can be also passed by `kwargs` of Python methods, **we encourage to use the dictionary to designate the parameters because it is easier to manage, modularize, and store parameters in certain formats like JSON.**\n",
    "\n",
    "For XR-Linear models, the default values and skeleton of the parameters can be revealed and generated by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ddc9bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      " \"train_params\": {\r\n",
      "  \"__meta__\": {\r\n",
      "   \"class_fullname\": \"pecos.xmc.xlinear.model###XLinearModel.TrainParams\"\r\n",
      "  },\r\n",
      "  \"mode\": \"full-model\",\r\n",
      "  \"ranker_level\": 1,\r\n",
      "  \"nr_splits\": 16,\r\n",
      "  \"min_codes\": null,\r\n",
      "  \"shallow\": false,\r\n",
      "  \"rel_mode\": \"disable\",\r\n",
      "  \"rel_norm\": \"no-norm\",\r\n",
      "  \"hlm_args\": {\r\n",
      "   \"__meta__\": {\r\n",
      "    \"class_fullname\": \"pecos.xmc.base###HierarchicalMLModel.TrainParams\"\r\n",
      "   },\r\n",
      "   \"neg_mining_chain\": \"tfn\",\r\n",
      "   \"model_chain\": {\r\n",
      "    \"__meta__\": {\r\n",
      "     \"class_fullname\": \"pecos.xmc.base###MLModel.TrainParams\"\r\n",
      "    },\r\n",
      "    \"threshold\": 0.1,\r\n",
      "    \"max_nonzeros_per_label\": null,\r\n",
      "    \"solver_type\": \"L2R_L2LOSS_SVC_DUAL\",\r\n",
      "    \"Cp\": 1.0,\r\n",
      "    \"Cn\": 1.0,\r\n",
      "    \"max_iter\": 100,\r\n",
      "    \"eps\": 0.1,\r\n",
      "    \"bias\": 1.0,\r\n",
      "    \"threads\": -1,\r\n",
      "    \"verbose\": 0,\r\n",
      "    \"newton_eps\": 0.01\r\n",
      "   }\r\n",
      "  }\r\n",
      " },\r\n",
      " \"pred_params\": {\r\n",
      "  \"__meta__\": {\r\n",
      "   \"class_fullname\": \"pecos.xmc.xlinear.model###XLinearModel.PredParams\"\r\n",
      "  },\r\n",
      "  \"hlm_args\": {\r\n",
      "   \"__meta__\": {\r\n",
      "    \"class_fullname\": \"pecos.xmc.base###HierarchicalMLModel.PredParams\"\r\n",
      "   },\r\n",
      "   \"model_chain\": {\r\n",
      "    \"__meta__\": {\r\n",
      "     \"class_fullname\": \"pecos.xmc.base###MLModel.PredParams\"\r\n",
      "    },\r\n",
      "    \"only_topk\": 20,\r\n",
      "    \"post_processor\": \"l3-hinge\"\r\n",
      "   }\r\n",
      "  }\r\n",
      " },\r\n",
      " \"indexer_params\": {\r\n",
      "  \"__meta__\": {\r\n",
      "   \"class_fullname\": \"pecos.xmc.base###HierarchicalKMeans.TrainParams\"\r\n",
      "  },\r\n",
      "  \"nr_splits\": 16,\r\n",
      "  \"min_codes\": null,\r\n",
      "  \"max_leaf_size\": 100,\r\n",
      "  \"imbalanced_ratio\": 0.0,\r\n",
      "  \"imbalanced_depth\": 100,\r\n",
      "  \"spherical\": true,\r\n",
      "  \"seed\": 0,\r\n",
      "  \"kmeans_max_iter\": 20,\r\n",
      "  \"threads\": -1\r\n",
      " }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pecos.xmc.xlinear.train --generate-params-skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472517",
   "metadata": {},
   "source": [
    "### A2.2. Training Parameters for Hierarchial Models in XR-Linear\n",
    "\n",
    "Hierarchical models could have different parameters over layers. To have customized parameters for the hierarchical model, `hlm_args` needs to be designated in the parameter dictionary. The values of `model_chain` and `neg_mining_chain` in `hlm_args` can be **a single dictionary** of general parameters for all layers or **a list of dictinoaries** for specific parameters of individual layers.\n",
    "\n",
    "#### General Parameters for All Layers\n",
    "\n",
    "```\n",
    "train_params_l1 = XLinearModel.TrainParams.from_dict(\n",
    "    {\n",
    "        ...\n",
    "        \"hlm_args\": {\n",
    "            ...\n",
    "            \"neg_mining_chain\": \"tfn\", # Negative sampling scheme for all layers\n",
    "            \"model_chain\":{...},       # Parameters for all layers\n",
    "        }\n",
    "        ...\n",
    "    })\n",
    "```\n",
    "\n",
    "#### Specific Parameters of Individual Layers\n",
    "\n",
    "```\n",
    "train_params_l1 = XLinearModel.TrainParams.from_dict(\n",
    "    {\n",
    "        ...\n",
    "        \"hlm_args\": {\n",
    "            ...\n",
    "            \"neg_mining_chain\": [\n",
    "                \"tfn\",      # Negative sampling scheme for layer-0\n",
    "                \"tfn\",      # Negative sampling scheme for layer-1\n",
    "                \"tfn+man\",  # Negative sampling scheme for layer-2\n",
    "                ...\n",
    "            ],\n",
    "            \"model_chain\": [\n",
    "                {...}, # Parameters for layer-0\n",
    "                {...}, # Parameters for layer-1\n",
    "                {...}, # Parameters for layer-2\n",
    "                ...\n",
    "            ],\n",
    "        }\n",
    "        ...\n",
    "    })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f86b8a",
   "metadata": {},
   "source": [
    "### A2.3. Variety of Solvers\n",
    "\n",
    "The solver for optimization can be adjusted by the argument `solver_type` in the `train` function. PECOS currently provides the following solvers for training each matcher/ranker:\n",
    "\n",
    "* \"L2R_L2LOSS_SVC_DUAL\" (default): L2-regularized L2-loss Dual SVM\n",
    "* \"L2R_L1LOSS_SVC_DUAL\": : L2-regularized L1-loss Dual SVM\n",
    "* \"L2R_LR_DUAL\": L2-reguarlized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f26ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_l1_kwargs = XLinearModel.train(\n",
    "    X_trn, Y_trn,\n",
    "    C=cluster_chain,\n",
    "    threshold=0.1,\n",
    "    negative_sampling_scheme=\"tfn\",\n",
    "    solver_type=\"L2R_L1LOSS_SVC_DUAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "197926a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params_l1 = XLinearModel.TrainParams.from_dict(\n",
    "    {\n",
    "        \"hlm_args\": {\n",
    "            \"threshold\": 0.1,\n",
    "            \"neg_mining_chain\": \"tfn\",\n",
    "            \"model_chain\":{\n",
    "                \"solver_type\": \"L2R_L1LOSS_SVC_DUAL\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "xlm_l1_dict = XLinearModel.train(\n",
    "    X_trn, Y_trn,\n",
    "    C=cluster_chain,\n",
    "    train_params=train_params_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eddf91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics with L2R_L1LOSS_SVC_DUAL (by method kwargs)\n",
      "prec   = 83.65 77.27 72.01 67.67 63.91 60.53 57.33 54.53 52.03 49.66\n",
      "recall = 4.94 9.06 12.53 15.59 18.26 20.64 22.67 24.52 26.23 27.72\n",
      "\n",
      "Evaluation Metrics with L2R_L1LOSS_SVC_DUAL (by dictionary)\n",
      "prec   = 83.65 77.27 72.01 67.67 63.91 60.53 57.33 54.53 52.03 49.66\n",
      "recall = 4.94 9.06 12.53 15.59 18.26 20.64 22.67 24.52 26.23 27.72\n"
     ]
    }
   ],
   "source": [
    "Y_pred_l1_kwargs = xlm_l1_kwargs.predict(X_tst, beam_size=10, only_topk=10)\n",
    "Y_pred_l1_dict = xlm_l1_dict.predict(X_tst, beam_size=10, only_topk=10)\n",
    "metrics_l1_kwargs = smat_util.Metrics.generate(Y_tst, Y_pred_l1_kwargs, topk=10)\n",
    "metrics_l1_dict = smat_util.Metrics.generate(Y_tst, Y_pred_l1_dict, topk=10)\n",
    "\n",
    "print(\"Evaluation Metrics with L2R_L1LOSS_SVC_DUAL (by method kwargs)\")\n",
    "print(metrics_l1_kwargs)\n",
    "\n",
    "print(\"\\nEvaluation Metrics with L2R_L1LOSS_SVC_DUAL (by dictionary)\")\n",
    "print(metrics_l1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f481ed3",
   "metadata": {},
   "source": [
    "## Appendix 3: Cost-sensitive Learning\n",
    "\n",
    "PECOS supports to adjust the cost of each training instance. To enable cost-sensitive learning, we need to provide a **relevance matrix** `R_trn` with the same shape to the label matrix `Y_trn` for the argument `R`. When `R` is `None` (default), cost-sensitive learning is disable. \n",
    "\n",
    "Since PECOS models are usually hierarhical, costs for upper layers also need to be decided as the cost-sensitive learning mode by the argument `rel_mode`. Currently, PECOS supports the following cost-sensitive learning modes:\n",
    "\n",
    "* `\"disable\"` (default): The cost-sensitive learning is disable.\n",
    "* `\"induce\"`: Induce the costs into upper layers by the clustering chain.\n",
    "* `\"ranker-only\"`: Only apply cost-sensitive learning to the model in the last ranker layer without induction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382277f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An exmaple of using training label frequency scores as costs. \n",
    "import copy\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "R_trn = copy.deepcopy(Y_trn)\n",
    "\n",
    "# Training parameters for cost-sensitive learning.\n",
    "train_params_cost = XLinearModel.TrainParams.from_dict(\n",
    "    {\n",
    "        \"rel_mode\": \"induce\",\n",
    "        \"rel_norm\": \"l1\",\n",
    "        \"hlm_args\": {\n",
    "            \"neg_mining_chain\": \"tfn\",\n",
    "            \"model_chain\":\n",
    "                [\n",
    "                    {\n",
    "                        \"threshold\": 0.1,\n",
    "                        \"Cp\": 1.0,\n",
    "                        \"Cn\": 1.0,\n",
    "                    },\n",
    "                    {\n",
    "                        \"threshold\": 0.1,\n",
    "                        \"Cp\": 8.0,\n",
    "                        \"Cn\": 1.0,\n",
    "                    },\n",
    "                    {\n",
    "                        \"threshold\": 0.1,\n",
    "                        \"Cp\": 4.0,\n",
    "                        \"Cn\": 1.0,\n",
    "                    },\n",
    "                    {\n",
    "                        \"threshold\": 0.1,\n",
    "                        \"Cp\": 4.0,\n",
    "                        \"Cn\": 1.0,\n",
    "                    },\n",
    "                ],\n",
    "        }\n",
    "    })\n",
    "        \n",
    "# Cost-sensitive learning.\n",
    "xlm_cost = XLinearModel.train(\n",
    "    X_trn, Y_trn,\n",
    "    C=cluster_chain,\n",
    "    R=R_trn,\n",
    "    train_params=train_params_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "559c15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics with Cost-sensitive Learning\n",
      "prec   = 84.93 80.15 74.50 69.32 64.73 61.05 57.68 54.52 51.70 49.10\n",
      "recall = 5.01 9.42 13.00 15.99 18.52 20.83 22.85 24.54 26.09 27.43\n",
      "\n",
      "Original Evaluation Metrics\n",
      "prec   = 84.36 78.20 72.67 68.03 63.85 60.23 56.86 53.85 51.07 48.63\n",
      "recall = 4.99 9.17 12.65 15.67 18.28 20.56 22.50 24.21 25.74 27.15\n"
     ]
    }
   ],
   "source": [
    "Y_pred_cost = xlm_cost.predict(X_tst, beam_size=10, only_topk=10)\n",
    "metrics_cost = smat_util.Metrics.generate(Y_tst, Y_pred_cost, topk=10)\n",
    "print(\"Evaluation Metrics with Cost-sensitive Learning\")\n",
    "print(metrics_cost)\n",
    "print(\"\\nOriginal Evaluation Metrics\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
