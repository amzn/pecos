{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee4c624-46ff-4a69-b315-097a4a471737",
   "metadata": {},
   "source": [
    "# How to Leverage Transformers in PECOS\n",
    "\n",
    "Extreme multi-label text classification (XMC) seeks to find relevant labels from an\n",
    "extreme large label collection for a given text input.\n",
    "The current state of the art result on XMC benchmarks are established by **XR-Transformer** [[NeurIPS21](https://arxiv.org/pdf/2110.00685.pdf)], which leverages recursively fine-tuned transformer encoders in text feature extaction.\n",
    "\n",
    "In this section, we will demostrate how you can use XR-Transformer to solve the XMC problems.\n",
    "\n",
    "### Download dataset and fine-tuned Transformer encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d380a9b4-dcf4-4fab-b07f-e9e0c38a15b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 21:39:55 URL:https://ia802308.us.archive.org/21/items/pecos-dataset/xmc-base/wiki10-31k.tar.gz [162277861/162277861] -> \"wiki10-31k.tar.gz\" [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmc-base/wiki10-31k/output-items.txt\n",
      "xmc-base/wiki10-31k/tfidf-attnxml\n",
      "xmc-base/wiki10-31k/tfidf-attnxml/X.trn.npz\n",
      "xmc-base/wiki10-31k/tfidf-attnxml/X.tst.npz\n",
      "xmc-base/wiki10-31k/X.trn.txt\n",
      "xmc-base/wiki10-31k/X.tst.txt\n",
      "xmc-base/wiki10-31k/Y.trn.npz\n",
      "xmc-base/wiki10-31k/Y.trn.txt\n",
      "xmc-base/wiki10-31k/Y.tst.npz\n",
      "xmc-base/wiki10-31k/Y.tst.txt\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_encoder\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_encoder/pytorch_model.bin\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_encoder/config.json\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/C.npz\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/param.json\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_model\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_tokenizer\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_tokenizer/tokenizer_config.json\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_tokenizer/special_tokens_map.json\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_tokenizer/vocab.txt\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder/text_tokenizer/tokenizer.json\n",
      "./work_dir/xr-transformer-encoder/wiki10-31k/bert/param.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATASET=\"wiki10-31k\"\n",
    "wget -nv -nc https://archive.org/download/pecos-dataset/xmc-base/${DATASET}.tar.gz\n",
    "tar --skip-old-files -zxf ${DATASET}.tar.gz \n",
    "find xmc-base/${DATASET}/*\n",
    "wget -q https://archive.org/download/xr-transformer-demos/${DATASET}-bert.tar.gz\n",
    "mkdir -p ./work_dir/xr-transformer-encoder\n",
    "tar -zxf ./${DATASET}-bert.tar.gz -C ./work_dir/xr-transformer-encoder\n",
    "find ./work_dir/xr-transformer-encoder/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c18b9-5356-4385-bffb-8b8c18d7ae06",
   "metadata": {},
   "source": [
    "## Outline in this Session\n",
    "\n",
    "  1. XR-Transformer Overview\n",
    "  2. Hands on training and evaluation\n",
    "  3. How to customize the parameter settings\n",
    "  4. Command line interface tools\n",
    "  5. Example pf using XR-Transformer on your custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e6dce-80eb-48f7-8604-f1695f04c878",
   "metadata": {},
   "source": [
    "## 1. XR-Transformer Overview\n",
    "\n",
    "## 1.1 Benchmarking XR-Transformer on public XMC datasets\n",
    "\n",
    "A comparison of Precision@1,3,5 and training time on 3 public XMC benchmarking datasets.\n",
    "\n",
    "PECOS XR-Transformer achieves the highgest accuracy while taking significantly less time to train (20-50X faster than X-Transformer).\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"imgs/xrtransformer_prec135.png\" width=\"90%\"/> </td>\n",
    "<td> <img src=\"imgs/xrtransformer_trainingtime.png\" width=\"80%\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "## 1.2 Training Procedures\n",
    "\n",
    "One important thing to note is that XR-Transformer leverages multi-resolution fine-tuning to allow tuning from easy to hard tasks. The training can be separated into three steps:\n",
    "\n",
    "* **Step1**: Label features are computed and are used to build preliminary hierarchical label tree (HLT).\n",
    "* **Step2**: Fine-tune the transformer encoder on the chosen levels of the preliminary HLT.\n",
    "* **Step3**: Concatenate final instance embeddings and sparse features and train the linear rankers on the refined HLT.\n",
    "\n",
    "<div> <br/><img src=\"imgs/pecos_xrtransformer.png\" width=\"70%\"/> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e63c1-1d08-49c0-b060-1aed92a76159",
   "metadata": {},
   "source": [
    "## 2. Hands on training and evaluation\n",
    "### 2.1 Data Loading\n",
    "\n",
    "XR-Transformer model takes both raw text as well as text numerical features (such as TFIDF) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99f59a5-b1f9-4d73-b09e-cf129923eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from pecos.utils import smat_util, logging_util\n",
    "\n",
    "# set logging level to WARNING(1)\n",
    "# you can change this to INFO(2) or DEBUG(3) if you'd like to see more logging\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging_util.setup_logging_config(level=1)\n",
    "\n",
    "# load training data\n",
    "X_feat_trn = smat_util.load_matrix(\"xmc-base/wiki10-31k/tfidf-attnxml/X.trn.npz\", dtype=np.float32)\n",
    "Y_trn = smat_util.load_matrix(\"xmc-base/wiki10-31k/Y.trn.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/wiki10-31k/X.trn.txt\", 'r') as fin:\n",
    "    X_txt_trn = [xx.strip() for xx in fin.readlines()]\n",
    "\n",
    "# load test data\n",
    "X_feat_tst = smat_util.load_matrix(\"xmc-base/wiki10-31k/tfidf-attnxml/X.tst.npz\", dtype=np.float32)\n",
    "Y_tst = smat_util.load_matrix(\"xmc-base/wiki10-31k/Y.tst.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/wiki10-31k/X.tst.txt\", 'r') as fin:\n",
    "    X_txt_tst = [xx.strip() for xx in fin.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a6873-2fcf-4a37-9d43-166dbf3b689d",
   "metadata": {},
   "source": [
    "### 2.2 Model Training and Evaluation\n",
    "\n",
    "In this section, we will compare the performance of three models:\n",
    "1. XR-Linear model with only sparse TF-IDF features\n",
    "2. XR-Transformer model without fine-tuning\n",
    "3. XR-Transformer model with fine-tuning\n",
    "\n",
    "XR-Transformer parameters for 6 public XMC benchmark datasets (i.e. `Eurlex-4K`, `Wiki10-31K`,\n",
    "`AmazonCat-13K`, `Wiki-500K`, `Amazon-670K`, `Amazon-3M`) are released. For this turoiral we will be using `Wiki10-31K` with `bert-base-uncased` encoder as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4d3ab3-31ec-4e10-8c1d-6bb30bdfbcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pecos.xmc.xtransformer.model import XTransformer\n",
    "\n",
    "# get XR-Transformer training params\n",
    "param_url = \"https://raw.githubusercontent.com/amzn/pecos/mainline/examples/xr-transformer-neurips21/params/wiki10-31k/bert/params.json\"\n",
    "params = json.loads(requests.get(param_url).text)\n",
    "    \n",
    "wiki31k_train_params = XTransformer.TrainParams.from_dict(params[\"train_params\"])\n",
    "wiki31k_pred_params = XTransformer.PredParams.from_dict(params[\"pred_params\"])\n",
    "\n",
    "# you can view the detailed parameter setting via\n",
    "#print(json.dumps(wiki31k_train_params.to_dict(), indent=True))\n",
    "#print(json.dumps(wiki31k_pred_params.to_dict(), indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f98b0f-cf5b-4844-a029-48ea339019f5",
   "metadata": {},
   "source": [
    "#### Baseline 1: XR-Linear\n",
    "Let's train a XR-Linear model on the TF-IDF features using the same hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2394cb6e-51f0-4485-85c2-74158445dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Linear model\n",
      "prec   = 84.96 81.82 76.30 70.70 65.67 61.46 57.92 54.63 51.77 49.16\n",
      "recall = 5.02 9.66 13.40 16.41 18.91 21.12 23.09 24.76 26.30 27.64\n"
     ]
    }
   ],
   "source": [
    "# construct label hierarchy\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "cluster_chain = Indexer.gen(\n",
    "    LabelEmbeddingFactory.create(Y_trn, X_feat_trn, method=\"pifa\"),\n",
    "    train_params=wiki31k_train_params.refined_indexer_params,\n",
    ")\n",
    "\n",
    "# train XR-Linear model\n",
    "from pecos.xmc.xlinear import XLinearModel\n",
    "xlm = XLinearModel.train(\n",
    "    X_feat_trn,\n",
    "    Y_trn,\n",
    "    C=cluster_chain,\n",
    "    train_params=wiki31k_train_params.ranker_params,\n",
    "    pred_params=wiki31k_pred_params.ranker_params,\n",
    ")\n",
    "\n",
    "# predict on test set with XR-Linear model\n",
    "P_xlm = xlm.predict(X_feat_tst)\n",
    "\n",
    "# compute metrics using ground truth\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xlm)\n",
    "print(\"Evaluation metrics of XR-Linear model\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7618e-7302-4486-9d82-00e95ec9a61c",
   "metadata": {},
   "source": [
    "#### Baseline 2: XR-Transformer without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24344e15-e183-45ad-abda-7239b6d5e144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Transformer (not fine-tuned)\n",
      "prec   = 85.22 82.55 77.26 72.15 67.42 63.13 59.33 56.08 53.02 50.24\n",
      "recall = 5.05 9.76 13.58 16.74 19.41 21.68 23.64 25.41 26.92 28.22\n"
     ]
    }
   ],
   "source": [
    "# define the problem\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt_trn, Y_trn, X_feat=X_feat_trn)\n",
    "\n",
    "# disable fine-tuning, directly use pre-trained bert model from huggingface\n",
    "wiki31k_train_params.do_fine_tune = False\n",
    "\n",
    "# train XR-Transformer (without fine-tuning)\n",
    "# this will be slow on CPU only machine\n",
    "xrt_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    train_params=wiki31k_train_params,\n",
    "    pred_params=wiki31k_pred_params,\n",
    ")\n",
    "\n",
    "# predict and compute metrics\n",
    "P_xrt_pretrained = xrt_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_pretrained)\n",
    "print(\"Evaluation metrics of XR-Transformer (not fine-tuned)\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3745a-2dc7-47e8-b3b4-c8c4ce331e05",
   "metadata": {},
   "source": [
    "#### Model: XR-Transformer\n",
    "For demo purpose, let's disable fine-tuning and load an already fine-tuned encoder directly (i.e. skip step 1&2).\n",
    "\n",
    "End-to-end training of XR-Transformer on **Wiki10-31K** dataset will take around 30min on **p3.16xlarge** instance.\n",
    "If you are running this on equivalent or more powerful machine, you can also turn on `DO_FINE_TUNE_NOW` and train XR-Transformer end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbada274-cfe3-4ef4-a5fe-448c00c3e6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Transformer\n",
      "prec   = 87.95 83.54 78.79 73.95 69.43 65.14 61.08 57.70 54.63 51.97\n",
      "recall = 5.25 9.89 13.84 17.14 19.99 22.36 24.35 26.16 27.73 29.21\n"
     ]
    }
   ],
   "source": [
    "DO_FINE_TUNE_NOW = False\n",
    "\n",
    "if DO_FINE_TUNE_NOW:\n",
    "    wiki31k_train_params.do_fine_tune = True\n",
    "else:\n",
    "    # skip fine-tuning and use existing fine-tuned encoder\n",
    "    wiki31k_train_params.do_fine_tune = False\n",
    "    wiki31k_train_params.matcher_params_chain[0].init_model_dir = \"./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder\"\n",
    "\n",
    "# this will be slow on CPU only machine\n",
    "xrt_fine_tuned = XTransformer.train(\n",
    "    prob,\n",
    "    clustering=cluster_chain,\n",
    "    train_params=wiki31k_train_params,\n",
    "    pred_params=wiki31k_pred_params,\n",
    ")\n",
    "\n",
    "P_xrt_fine_tuned = xrt_fine_tuned.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_fine_tuned, topk=10)\n",
    "print(\"Evaluation metrics of XR-Transformer\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7f85b-96f7-4801-ab86-410c5f0115a1",
   "metadata": {},
   "source": [
    "### 2.3 Save and load model, get transformer embeddings\n",
    "Note you can pass keyword arguments of `XLinear.load` to `XTransformer.load` such as `is_predict_only`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233050a5-0398-48d0-8ef8-fc58232de669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"./work_dir/my_xrt\"\n",
    "xrt_fine_tuned.save(model_folder)\n",
    "del xrt_fine_tuned\n",
    "xrt_fine_tuned = XTransformer.load(model_folder, is_predict_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8e6fd-7755-4068-acdb-bae212c26c4f",
   "metadata": {},
   "source": [
    "For BERT model, ebmeddings are from the [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd7bcd2-0798-4bdd-8460-9e7f114d71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated test embedding type=<class 'numpy.ndarray'> with shape=(6616, 768)\n"
     ]
    }
   ],
   "source": [
    "X_emb_tst = xrt_fine_tuned.encode(\n",
    "    X_txt_tst,\n",
    "    batch_size=256,\n",
    "    batch_gen_workers=8,\n",
    ")\n",
    "print(f\"Generated test embedding type={type(X_emb_tst)} with shape={X_emb_tst.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb0589-54a2-41fa-bb25-0f9343f1a9bc",
   "metadata": {},
   "source": [
    "### 2.4 Training without TFIDF features\n",
    "\n",
    "The XR-Transformer module can also be used with only text features when numerical features like TFIDF are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b1860c-3bc7-4c46-ba57-3bab8ebd012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics of XR-Transformer (without TFIDF)\n",
      "prec   = 86.23 81.57 76.04 70.60 65.52 61.38 57.65 54.31 51.45 48.81\n",
      "recall = 5.11 9.61 13.28 16.31 18.80 21.00 22.90 24.54 26.06 27.38\n"
     ]
    }
   ],
   "source": [
    "prob_only_text = MLProblemWithText(X_txt_trn, Y_trn)\n",
    "wiki31k_train_params.do_fine_tune = False\n",
    "wiki31k_train_params.matcher_params_chain[0].init_model_dir = \"./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder\"\n",
    "\n",
    "# this will be slow on CPU only machine\n",
    "xrt_only_text = XTransformer.train(\n",
    "    prob_only_text,\n",
    "    clustering=cluster_chain,\n",
    "    train_params=wiki31k_train_params,\n",
    "    pred_params=wiki31k_pred_params,\n",
    ")\n",
    "\n",
    "P_xrt_only_text = xrt_only_text.predict(X_txt_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_xrt_only_text, topk=10)\n",
    "print(\"Evaluation metrics of XR-Transformer (without TFIDF)\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ac8e2-555d-4bc2-a187-6a590bc7944c",
   "metadata": {},
   "source": [
    "## 3 How to customize the parameter settings\n",
    "For your custom dataset, it is recommended to start from the pre-defined parameters or the default value and make proper modifications based on the specific problem.\n",
    "\n",
    "### 3.1 Training Parameters of XTransformer.\n",
    "\n",
    "```\n",
    "xrt_train_params = XTransformer.TrainParams.from_dict(\n",
    "{\n",
    " \"do_fine_tune\": [true/false],                   # if true, do encoder fine-tuning\n",
    " \"only_encoder\": [true/false],                   # if true, skip linear ranker training\n",
    " \"max_match_clusters\": INT                       # max label resolution to fine-tune encoder on\n",
    " \"preliminary_indexer_params\": {...},            # (HierarchicalKMeans.TrainParams) parameters to construct preliminary HLT \n",
    " \"refined_indexer_params\": {...},                # (HierarchicalKMeans.TrainParams) parameters to construct refined HLT \n",
    " \"matcher_params_chain\": [                       # fine-tuning parameters. Can be dict or list of dict. If dict, all layers will share the same setting\n",
    "   {...},                                        # (TransformerMatcher.TrainParams) fine-tuning parameters for layer-0\n",
    "   {...},                                        # (TransformerMatcher.TrainParams) fine-tuning parameters for layer-1\n",
    "   ...\n",
    " ],\n",
    " \"ranker_params\": {...},                         # (XLinearModel.TrainParams) ranker training parameters\n",
    "}\n",
    ")\n",
    "```\n",
    "\n",
    "You can get the training and prediction parameters filled with default values by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489c8bab-f7d6-4445-b4b8-c44097cb8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = XTransformer.TrainParams.from_dict({}, recursive=True)\n",
    "pred_params = XTransformer.PredParams.from_dict({}, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58825d-9e85-4556-bf7f-1a9f1ddde8c4",
   "metadata": {},
   "source": [
    "Detailed control over each layer's fine-tuning task is done through `matcher_params_chain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ab552c-1845-47fd-9eb8-1d3c13bb9bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"__meta__\": {\n",
      "  \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.TrainParams\"\n",
      " },\n",
      " \"model_shortcut\": \"bert-base-cased\",\n",
      " \"negative_sampling\": \"tfn\",\n",
      " \"loss_function\": \"squared-hinge\",\n",
      " \"bootstrap_method\": \"linear\",\n",
      " \"lr_schedule\": \"linear\",\n",
      " \"threshold\": 0.1,\n",
      " \"hidden_dropout_prob\": 0.1,\n",
      " \"batch_size\": 8,\n",
      " \"batch_gen_workers\": 4,\n",
      " \"max_active_matching_labels\": null,\n",
      " \"max_num_labels_in_gpu\": 65536,\n",
      " \"max_steps\": 0,\n",
      " \"max_no_improve_cnt\": -1,\n",
      " \"num_train_epochs\": 5,\n",
      " \"gradient_accumulation_steps\": 1,\n",
      " \"weight_decay\": 0,\n",
      " \"max_grad_norm\": 1.0,\n",
      " \"learning_rate\": 0.0001,\n",
      " \"adam_epsilon\": 1e-08,\n",
      " \"warmup_steps\": 0,\n",
      " \"logging_steps\": 50,\n",
      " \"save_steps\": 100,\n",
      " \"cost_sensitive_ranker\": false,\n",
      " \"pre_tokenize\": true,\n",
      " \"pre_tensorize_labels\": true,\n",
      " \"use_gpu\": true,\n",
      " \"eval_by_true_shorlist\": false,\n",
      " \"checkpoint_dir\": \"\",\n",
      " \"cache_dir\": \"\",\n",
      " \"init_model_dir\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(train_params.matcher_params_chain.to_dict(), indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b7124-00b2-4275-af9a-dbdcbcab39d5",
   "metadata": {},
   "source": [
    "### 3.2 Getting the pre-trained models\n",
    "\n",
    "There are two ways to provide pre-trained Transformer encoder:\n",
    "* **Download from huggingface repo** (https://huggingface.co/models): pre-trained model name provided in `model_shortcut` (under `XTransformer.TrainParams.matcher_params_chain`) will be automatically downloaded. (e.x. `bert-base-uncased`)\n",
    "* **Load your custom model from local disk**: model path provided by `init_model_dir`. Model should be loadable through `TransformerMatcher.load()`\n",
    "\n",
    "Note that both `model_shortcut` and `init_model_dir` will only be used in the first fine-tuning layer, as the later ones will just continue on the final state from parent encoder.\n",
    "\n",
    "A simple example if you want to construct your custom pre-trained model for XR-Transformer fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54abca4-d07b-4cef-a68d-6b1b40d83280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at work_dir/my_pre_trained_model/text_encoder were not used when initializing BertForXMC: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/13/2022 21:48:31 - WARNING - pecos.xmc.xtransformer.matcher - XMC text_model of BertForXMC not initialized from pre-trained model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pecos.xmc.xtransformer.matcher.TransformerMatcher'> model loaded with encoder_type=bert num_labels=2\n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc.xtransformer.matcher import TransformerMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "init_model_dir = \"work_dir/my_pre_trained_model\"\n",
    "\n",
    "# example to use your own pre-trained model, here we use huggingface model as an example\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "my_encoder = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ...\n",
    "# do my own modification/tuning/etc\n",
    "# ...\n",
    "\n",
    "# save my own model to disk\n",
    "my_tokenizer.save_pretrained(f\"{init_model_dir}/text_tokenizer\")\n",
    "my_encoder.save_pretrained(f\"{init_model_dir}/text_encoder\")\n",
    "\n",
    "# then the `work_dir` can be fed as `init_model_dir` as initial model.\n",
    "# Sanity check: if this dir can be loaded via TransformerMatcher.load(*)\n",
    "matcher = TransformerMatcher.load(init_model_dir)\n",
    "print(f\"{matcher.__class__} model loaded with encoder_type={matcher.model_type} num_labels={matcher.nr_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8cd72-12e1-4aa3-940e-24b36095e440",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  4. Command line interface tools\n",
    "You can achieve the same functionalities with the provided command line tools.\n",
    "\n",
    "Although we provide basic functionalities to supply training and prediction parameters in the CLI tool `pecos.xmc.xtransformer.train`, `pecos.xmc.xtransformer.predict` and `pecos.xmc.xtransformer.encode`,\n",
    "you should supply parameters via a JSON file if you want full control over the training/prediction process.\n",
    "\n",
    "Similar to the python interface, you can also generate a `.json` file with all of the parameters that you can edit and fill in via\n",
    "```bash\n",
    "python3 -m pecos.xmc.xtransformer.train --generate-params-skeleton &> params.json\n",
    "```\n",
    "\n",
    "After filling in the desired parameters into `params.json`, the training can be done end2end via:\n",
    "```bash\n",
    "python3 -m pecos.xmc.xtransformer.train \\\n",
    "    -t ${T_path} \\\n",
    "    -x ${X_path} \\\n",
    "    -y ${Y_path} \\\n",
    "    -m ${model_dir} \\\n",
    "    --params-path params.json\n",
    "\n",
    "python3 -m pecos.xmc.xtransformer.predict \\\n",
    "    -t ${Tt_path} \\\n",
    "    -x ${Xt_path} \\\n",
    "    -m ${model_dir} \\\n",
    "    -o ${Pt_path}\n",
    "```\n",
    "where\n",
    "* `T_path` and `Tt_path` are the paths to the input text file of the training/test instances. Text files with `N`/`Nt` lines where each line is the text feature of the corresponding training/test instance.\n",
    "* `X_path` and `Xt_path` are the paths to the CSR npz or Row-majored npy files of the training/test feature matrices with shape `(N, d)` and `(Nt, d)`.\n",
    "  * Note that you can use the PECOS built in text preprocessing/vectorizing module [pecos.utils.featurization.text.preprocess](https://github.com/amzn/pecos/tree/mainline/pecos/utils/featurization/text) to generate numerical features if you do not already have them.\n",
    "  * Usually providing instance numerical features is recommended. However, if you choose not to provide numerical features, `code-path` or `label-feat-path` is required to generate the hierarchical label trees.\n",
    "* `Y_path` and `Yt_path` are the paths to the CSR npz files of the training/test label matrices with shape `(N, L)` and `(Nt, L)`.\n",
    "* `model_dir` is the path to the model folder where the trained model will be saved to, will be created if not exist.\n",
    "* `Pt_path` is the path to save the prediction label matrix with shape `(Nt, L)`\n",
    "\n",
    "To get the evaluation metrics for top-10 predictions:\n",
    "```bash\n",
    "python3 -m pecos.xmc.xlinear.evaluate \\\n",
    "    -y ${Yt_path} \\\n",
    "    -p ${Pt_path} \\\n",
    "    -k 10\n",
    "```\n",
    "You can also get the fine-tuned text embeddings via:\n",
    "```bash\n",
    "python3 -m pecos.xmc.xtransformer.encode \\\n",
    "    -t ${Tt_path} \\\n",
    "    -m ${model_dir} \\\n",
    "    -o ${Emb_path}\n",
    "```\n",
    "\n",
    "where\n",
    "* `Emb_path` is the path to save the prediction label matrix with shape `(Nt, hidden_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1293642-87cb-4148-b206-04fc38340c9c",
   "metadata": {},
   "source": [
    "## 5. Example: Use XR-Transformer for your custom dataset\n",
    "This section demostrates how you can use XR-Transformer on your custom dataset.\n",
    "\n",
    "**Note**: The data used here is a dummy dataset only for demo purposes, therefore we don't expect sensical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43657390-f634-4ec9-b940-92ed511a0f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-13 21:48:32 URL:https://ia601500.us.archive.org/21/items/text2text_demo.tar.gz/text2text_demo.tar.gz [674/674] -> \"text2text_demo.tar.gz\" [1]\n",
      "text2text_demo/output-labels.txt\n",
      "text2text_demo/testing-data.txt\n",
      "text2text_demo/training-data.txt\n"
     ]
    }
   ],
   "source": [
    "! wget -nv -nc https://archive.org/download/text2text_demo.tar.gz/text2text_demo.tar.gz\n",
    "! tar --skip-old-files -zxf text2text_demo.tar.gz\n",
    "! find text2text_demo/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9af6ab-3f0e-4d0f-a059-6e699b8792df",
   "metadata": {},
   "source": [
    "First format your input data into two files `training-data.txt` and `output-labels.txt`.\n",
    "\n",
    "Each line of `output-labels.txt` corresponds to the text representation of a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe80fa9-e6a3-455a-8f1e-26d181c58e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence researchers\n",
      "Computability theorists\n",
      "British computer scientists\n",
      "Machine learning researchers\n",
      "Turing Award laureates\n",
      "Deep Learning\n"
     ]
    }
   ],
   "source": [
    "! cat ./text2text_demo/output-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f5ac8-b738-432d-806c-22e1f1d369e0",
   "metadata": {},
   "source": [
    "The `training-data.txt` stores input corpus and training signals. Each line in the file consists of two elements that represent the comma-separated label IDs and the input text of a data instance: \n",
    "\n",
    "<p style=\"text-align: center;\"><i>\n",
    "label_idx1,label_idx2,... &lt;TAB&gt; instance_text</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d785ef4-6b0f-4827-a9c4-8a5ffe9aea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\tAlan Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\n",
      "0,2,3\tHinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks.\n",
      "3,4,5\tHinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on artificial intelligence and deep learning.\n",
      "0,3,5\tYoshua Bengio is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning.\n"
     ]
    }
   ],
   "source": [
    "! cat ./text2text_demo/training-data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2465b7-5f5c-4c88-b53f-114bdef3ef2e",
   "metadata": {},
   "source": [
    "First parse the `training-data.txt` into training corpus and label matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7006bb0-15d4-46b3-aaea-38ca484eea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed training corpus len=4, training label matrix with shape=(4, 6) and nnz=12\n"
     ]
    }
   ],
   "source": [
    "from pecos.utils.featurization.text.preprocess import Preprocessor\n",
    "\n",
    "parsed_result = Preprocessor.load_data_from_file(\n",
    "    \"./text2text_demo/training-data.txt\",\n",
    "    \"./text2text_demo/output-labels.txt\",\n",
    ")\n",
    "Y = parsed_result[\"label_matrix\"]\n",
    "X_txt = parsed_result[\"corpus\"]\n",
    "\n",
    "print(f\"Constructed training corpus len={len(X_txt)}, training label matrix with shape={Y.shape} and nnz={Y.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b130f88-f40f-4bca-8675-7c454e5fd774",
   "metadata": {},
   "source": [
    "Build TF-IDF model with training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54c8b14-8fc0-49b6-999d-25b28d747755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed training feature matrix with shape=(4, 125) and nnz=151\n"
     ]
    }
   ],
   "source": [
    "vectorizer_config = {\n",
    "    \"type\": \"tfidf\",\n",
    "    \"kwargs\": {\n",
    "      \"base_vect_configs\": [\n",
    "        {\n",
    "          \"ngram_range\": [1, 2],\n",
    "          \"max_df_ratio\": 0.98,\n",
    "          \"analyzer\": \"word\",\n",
    "        },\n",
    "      ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tfidf_model = Preprocessor.train(X_txt, vectorizer_config)\n",
    "X_feat = tfidf_model.predict(X_txt)\n",
    "\n",
    "print(f\"Constructed training feature matrix with shape={X_feat.shape} and nnz={X_feat.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae807f1-f34c-4d11-9e5a-50e87ed42443",
   "metadata": {},
   "source": [
    "Train XR-Transformer with all default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "058ffcdf-54c7-4eff-af97-2418d5ff2b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForXMC: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/ec2-user/miniconda3/envs/tutorial_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc.xtransformer.model import XTransformer\n",
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt, Y, X_feat=X_feat)\n",
    "custom_xtf = XTransformer.train(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f8eae-47c7-4361-bed7-cdc5894e03a3",
   "metadata": {},
   "source": [
    "Save tfidf model and XR-Transformer model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c270281-c729-44f1-a9b4-a8765048e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "custom_model_dir = \"work_dir/custom_model\"\n",
    "os.makedirs(custom_model_dir, exist_ok=True)\n",
    "\n",
    "tfidf_model.save(f\"{custom_model_dir}/tfidf_model\")\n",
    "custom_xtf.save(f\"{custom_model_dir}/xrt_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24024b-7d19-44c7-9d12-c9e496f23526",
   "metadata": {},
   "source": [
    "Load tfidf model and XR-Transformer model from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318e04f4-be60-4b38-9c37-3b04092bef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_xtf = XTransformer.load(f\"{custom_model_dir}/xrt_model\")\n",
    "tfidf_model = Preprocessor.load(f\"{custom_model_dir}/tfidf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a521df7-d595-43ac-8412-1060eaf2d5a9",
   "metadata": {},
   "source": [
    "Predict on a test input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39922231-b05e-4a1c-b5ba-f23360d66adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: In 1989, Yann LeCun et al. applied the standard backpropagation algorithm on neural networks for hand digit recognition.\n",
      "Predicted label: Machine learning researchers\n",
      "Predicted score: 0.7240481376647949\n"
     ]
    }
   ],
   "source": [
    "test_input = [\"In 1989, Yann LeCun et al. applied the standard backpropagation algorithm on neural networks for hand digit recognition.\"]\n",
    "\n",
    "P = custom_xtf.predict(\n",
    "    test_input,\n",
    "    X_feat=tfidf_model.predict(test_input),\n",
    "    only_topk=1\n",
    ")\n",
    "\n",
    "with open(\"./text2text_demo/output-labels.txt\", 'r') as fin:\n",
    "    output_items = [ll.strip() for ll in fin.readlines()]\n",
    "\n",
    "for i, t in enumerate(test_input):\n",
    "    print(f\"Input text: {t}\")\n",
    "    print(f\"Predicted label: {output_items[P[i, :].indices[0]]}\")\n",
    "    print(f\"Predicted score: {P[i, :].data[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
