{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee4c624-46ff-4a69-b315-097a4a471737",
   "metadata": {},
   "source": [
    "# eXtreme Multi-label Ranking (XMR) with Transformers\n",
    "In many XMC applications, XR-Transformer is able to yield better performance than XR-Linear due to better extraction of semantic information. However, unlike the linear models, the training hyper-parameters need to be carefully set to achieve the best performance. Naively using the default setting will often lead to sub-optimal results.\n",
    "\n",
    "In this section, we will discuss about crucial components in training a good XR-Transformer model.\n",
    "\n",
    "### Install PECOS through Python PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install libpecos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e6dce-80eb-48f7-8604-f1695f04c878",
   "metadata": {},
   "source": [
    "## 1. Overview: Multi-Resolution Fine-tuning\n",
    "\n",
    "One important thing to note is that XR-Transformer leverages multi-resolution fine-tuning to allow tuning from easy to hard tasks. The training can be separated into three steps:\n",
    "\n",
    "* **Step1**: Label features are computed (usually via PIFA) and is used to build preliminary hierarchical label tree (HLT) via hierarchical k-means.\n",
    "* **Step2**: Fine-tune the transformer encoder on the chosen levels of the preliminary HLT.\n",
    "* **Step3**: Concatenate final instance embeddings and sparse features and train the linear rankers on the refined HLT.\n",
    "\n",
    "<div> <br/><img src=\"imgs/pecos_xrtransformer.png\" width=\"80%\"/> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a355aa-3f50-4471-9b8e-08538f3bc57e",
   "metadata": {},
   "source": [
    "## 2. Parameter structure\n",
    "\n",
    "Although we provide basic functionalities to supply training and prediction parameters in the CLI API `pecos.xmc.xtransformer.train`, `pecos.xmc.xtransformer.predict` and `pecos.xmc.xtransformer.encode`, for advanced users it is recommended to give parameters via JSON format.\n",
    "\n",
    "You can generate a `.json` file with all of the parameters that you can edit and fill in via\n",
    "```bash\n",
    "python3 -m pecos.xmc.xtransformer.train --generate-params-skeleton &> params.json\n",
    "```\n",
    "\n",
    "After filling in the desired parameters into `params.json`, the training can be done end2end via:\n",
    "```bash\n",
    "python3 -m pecos.xmc.xtransformer.train -t ${T_path} -x ${X_path} -y ${Y_path} -m ${model_dir} --params-path params.json\n",
    "```\n",
    "\n",
    "The high-level structure of the training and prediction parameters for XR-Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e47f23-8525-4588-ab44-97c8f52f6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Parameters of XTransformer.\n",
      "\n",
      "        preliminary_indexer_params (HierarchicalKMeans.TrainParams): params to generate preliminary hierarchial label tree.\n",
      "            ignored if clustering is given\n",
      "        refined_indexer_params (HierarchicalKMeans.TrainParams): params to generate refined hierarchial label tree.\n",
      "            ignored if fix_clustering is True\n",
      "        matcher_params_chain (TransformerMatcher.TrainParams or list): chain of params for TransformerMatchers.\n",
      "        ranker_params (XLinearModel.TrainParams): train params for linear ranker\n",
      "\n",
      "        do_fine_tune (bool, optional): if False, skip fine-tuning steps and directly use pre-trained transformer models.\n",
      "            Default True\n",
      "        only_encoder (bool, optional): if True, skip linear ranker training. Default False\n",
      "        fix_clustering (bool, optional): if True, use the same hierarchial label tree for fine-tuning and final prediction. Default false.\n",
      "        max_match_clusters (int, optional): max number of clusters on which to fine-tune transformer. Default 32768\n",
      "        \n",
      "Pred Parameters of XTransformer.\n",
      "\n",
      "        matcher_params_chain (TransformerMatcher.PredParams or list): chain of params for TransformerMatchers\n",
      "        ranker_params (XLinearModel.PredParams): pred params for linear ranker\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pecos.xmc.xtransformer.model import XTransformer\n",
    "from pecos.utils import logging_util\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "logging_util.setup_logging_config(level=1)\n",
    "\n",
    "print(XTransformer.TrainParams.__doc__)\n",
    "print(XTransformer.PredParams.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b84bd-5c52-425c-b31f-93eb7c6b5e49",
   "metadata": {},
   "source": [
    "We provide the fexibility to control almost every aspect of XR-Transformer taining, let's cover the main components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101dc60-0fed-47dd-8c2b-de3ecbc7bd97",
   "metadata": {},
   "source": [
    "### 2.1 Specify the Label Hierarchy\n",
    "\n",
    "The structure and construction of the preliminary HLT and the refined-HLT are controlled by `preliminary_indexer_params` and `refined_indexer_params`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f9523a-b7e5-49ff-9874-bcb21bb55eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Parameters of Hierarchical K-means.\n",
      "\n",
      "        nr_splits (int, optional): The out-degree of each internal node of the tree. Default is `16`.\n",
      "        min_codes (int): The number of direct child nodes that the top level of the hierarchy should have.\n",
      "        max_leaf_size (int, optional): The maximum size of each leaf node of the tree. Default is `100`.\n",
      "        spherical (bool, optional): True will l2-normalize the centroids of k-means after each iteration. Default is `True`.\n",
      "        seed (int, optional): Random seed. Default is `0`.\n",
      "        kmeans_max_iter (int, optional): Maximum number of iterations for each k-means problem. Default is `20`.\n",
      "        threads (int, optional): Number of threads to use. `-1` denotes all CPUs. Default is `-1`.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc.base import HierarchicalKMeans; print(HierarchicalKMeans.TrainParams.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c61c61-20ad-4a4f-93a3-85a736215310",
   "metadata": {},
   "source": [
    "Here is an example of the parameters related to label hierarchy in `wiki10-31k` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2759001f-413f-49be-bfad-38f58b9147d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"__meta__\": {\n",
      "  \"class_fullname\": \"pecos.xmc.base###HierarchicalKMeans.TrainParams\"\n",
      " },\n",
      " \"nr_splits\": 16,\n",
      " \"min_codes\": 128,\n",
      " \"max_leaf_size\": 16,\n",
      " \"spherical\": true,\n",
      " \"seed\": 10001,\n",
      " \"kmeans_max_iter\": 20,\n",
      " \"threads\": -1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pecos\n",
    "import requests\n",
    "import numpy as np\n",
    "from pecos.utils import smat_util\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "\n",
    "param_url = \"https://raw.githubusercontent.com/amzn/pecos/mainline/examples/xr-transformer-neurips21/params/wiki10-31k/bert/params.json\"\n",
    "params = json.loads(requests.get(param_url).text)\n",
    "    \n",
    "wiki31k_train_params = XTransformer.TrainParams.from_dict(params[\"train_params\"])\n",
    "wiki31k_pred_params = XTransformer.PredParams.from_dict(params[\"pred_params\"])\n",
    "\n",
    "print(json.dumps(wiki31k_train_params.preliminary_indexer_params.to_dict(), indent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90802b2f-55ef-4a64-9252-08f7a1e30bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preliminary HLT structure [128, 2048, 30938]\n"
     ]
    }
   ],
   "source": [
    "X_feat = smat_util.load_matrix(\"xmc-base/wiki10-31k/tfidf-attnxml/X.trn.npz\", dtype=np.float32)\n",
    "Y = smat_util.load_matrix(\"xmc-base/wiki10-31k/Y.trn.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/wiki10-31k/X.trn.txt\", 'r') as fin:\n",
    "    X_txt = [xx.strip() for xx in fin.readlines()]\n",
    "\n",
    "preliminary_hlt = Indexer.gen(\n",
    "    LabelEmbeddingFactory.create(Y, X_feat, method=\"pifa\"),\n",
    "    train_params=wiki31k_train_params.preliminary_indexer_params,\n",
    ")\n",
    "\n",
    "print(f\"Preliminary HLT structure {[c.shape[0] for c in preliminary_hlt]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11232266-fceb-464b-8b71-ceaf723d7b39",
   "metadata": {},
   "source": [
    "In this case the preliminiary HLT has 3 levels (128-2048-30938).\n",
    "As we choose the `max_match_clusters` to be `32768`, the fine-tuning will happen on all 3 levels of preliminary HLT.\n",
    "\n",
    "The preliminary HLT is usually constructed such that:\n",
    "* The initial fine-tuning task has low enough label resolution (i.e. < 1000 labels, in this case 128). This is to ensure Transformers can start from simple task to 'warm-up'.\n",
    "* The final fine-tuning task has high enough label resolution (controlled by `max_match_clusters`, in this case 32768). The is to ensure training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac812a91-024f-45cc-9d3c-f97b7f98ac94",
   "metadata": {},
   "source": [
    "### 2.2 Control fine-tuning at each level\n",
    "\n",
    "At each level of the fine-tuning task, user can independently specify the training parameters such as `loss_function`, `batch_size` and etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be25c125-3dc6-4ec7-b83f-ef289312778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Parameters of MLModel\n",
      "\n",
      "        model_shortcut (str): string of pre-trained model shortcut. Default 'bert-base-cased'\n",
      "        negative_sampling (str): negative sampling types. Default tfn\n",
      "        loss_function (str): type of loss function to use for transformer\n",
      "            training. Default 'squared-hinge'\n",
      "        bootstrap_method (str): algorithm to bootstrap text_model. If not None, initialize\n",
      "            TransformerMatcher projection layer with one of:\n",
      "                'linear' (default): linear model trained on final embeddings of parent layer\n",
      "                'inherit': inherit weights from parent labels\n",
      "        lr_schedule (str): learning rate schedule. See transformers.SchedulerType for details.\n",
      "            Default 'linear'\n",
      "\n",
      "        threshold (float): threshold to sparsify the model weights. Default 0.1\n",
      "        hidden_dropout_prob (float): hidden dropout prob in deep transformer models. Default 0.1\n",
      "        batch_size (int):  batch size for transformer training. Default 8\n",
      "        batch_gen_workers (int): number of workers for batch generation. Default 4\n",
      "        max_active_matching_labels (int): max number of active matching labels,\n",
      "            will sub-sample from existing negative samples if necessary. Default None\n",
      "            to ignore\n",
      "        max_num_labels_in_gpu (int): Upper limit on labels to put output layer in GPU.\n",
      "            Default 65536.\n",
      "        max_steps (int): if > 0: set total number of training steps to perform.\n",
      "            Override num-train-epochs. Default -1.\n",
      "        max_no_improve_cnt (int): if > 0, training will stop when this number of\n",
      "            validation steps result in no improvement. Default -1.\n",
      "        num_train_epochs (int): total number of training epochs to perform. Default 5\n",
      "        gradient_accumulation_steps (int): number of updates steps to accumulate\n",
      "            before performing a backward/update pass. Default 1.\n",
      "        weight_decay (float): weight decay rate for regularization. Default 0 to ignore\n",
      "        max_grad_norm (float): max gradient norm used for gradient clipping. Default 1.0\n",
      "        learning_rate (float): maximum learning rate for Adam. Default 5e-5\n",
      "        adam_epsilon (float): epsilon for Adam optimizer.Default 1e-8\n",
      "        warmup_steps (float): learning rate warmup over warmup-steps. Default 0\n",
      "        logging_steps (int): log training information every NUM updates steps. Default 50\n",
      "        save_steps (int): save checkpoint every NUM updates steps. Default 100\n",
      "\n",
      "        cost_sensitive_ranker (bool, optional): if True, use clustering count aggregating for ranker's cost-sensitive learnin\n",
      "            Default False\n",
      "        pre_tokenize (bool, optional): if True, will tokenize training instances before training\n",
      "            This could potentially accelerate batch-generation but increases memory cost.\n",
      "            Default False\n",
      "        use_gpu (bool, optional): whether to use GPU even if available. Default True\n",
      "        eval_by_true_shorlist (bool, optional): if True, will compute validation scores by true label\n",
      "            shortlisting at intermediat layer. Default False\n",
      "\n",
      "        checkpoint_dir (str): path to save training checkpoints. Default empty to use a temp dir.\n",
      "        cache_dir (str): dir to store the pre-trained models downloaded from\n",
      "            s3. Default empty to use a temp dir.\n",
      "        init_model_dir (str): path to load checkpoint of TransformerMatcher. If given,\n",
      "            start from the given checkpoint rather than downloading a\n",
      "            pre-trained model from S3. Default empty to ignore\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc.xtransformer.matcher import TransformerMatcher; print(TransformerMatcher.TrainParams.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc7480-d6a7-4c81-96cf-cb35aa823fc2",
   "metadata": {},
   "source": [
    "For the `wiki10-31k` model, we are fine-tuning the `bert-base-uncased` pre-trained model at 3 levels of the preliminary HLT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db634cb8-6763-411a-8f6e-6100ff1d4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== matcher_params_chain[0] (len=3) ==========\n",
      "{\n",
      " \"__meta__\": {\n",
      "  \"class_fullname\": \"pecos.xmc.xtransformer.matcher###TransformerMatcher.TrainParams\"\n",
      " },\n",
      " \"adam_epsilon\": 1e-08,\n",
      " \"batch_gen_workers\": 16,\n",
      " \"batch_size\": 32,\n",
      " \"bootstrap_method\": \"weighted-linear\",\n",
      " \"cache_dir\": \"\",\n",
      " \"checkpoint_dir\": \"\",\n",
      " \"cost_sensitive_ranker\": true,\n",
      " \"eval_by_true_shorlist\": false,\n",
      " \"gradient_accumulation_steps\": 1,\n",
      " \"hidden_dropout_prob\": 0.1,\n",
      " \"init_model_dir\": \"\",\n",
      " \"learning_rate\": 5e-05,\n",
      " \"logging_steps\": 50,\n",
      " \"loss_function\": \"weighted-squared-hinge\",\n",
      " \"lr_schedule\": \"linear\",\n",
      " \"max_active_matching_labels\": 1000,\n",
      " \"max_grad_norm\": 1.0,\n",
      " \"max_no_improve_cnt\": -1,\n",
      " \"max_num_labels_in_gpu\": 65536,\n",
      " \"max_steps\": 1000,\n",
      " \"model_shortcut\": \"bert-base-uncased\",\n",
      " \"negative_sampling\": \"tfn+man\",\n",
      " \"num_train_epochs\": 10,\n",
      " \"pre_tensorize_labels\": true,\n",
      " \"pre_tokenize\": true,\n",
      " \"save_steps\": 200,\n",
      " \"threshold\": 0.001,\n",
      " \"use_gpu\": true,\n",
      " \"warmup_steps\": 100,\n",
      " \"weight_decay\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*10, f\"matcher_params_chain[0] (len={len(wiki31k_train_params.matcher_params_chain)})\", \"=\"*10)\n",
    "print(json.dumps(wiki31k_train_params.matcher_params_chain[0].to_dict(), sort_keys=True, indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b1b30-a300-42e2-b8e0-295c426fe655",
   "metadata": {},
   "source": [
    "Though the best parameters may vary a lot for different tasks, there are some common notes you should alwasy\n",
    "* It's recommended to finish at least one epoch at each level. This will allow the model to visit the label matrix at least once.\n",
    "  * i.e. `max_steps * batch_size * num_gpus > num_instances` (if `max_steps` is null, it will be infered from `num_train_epochs`)\n",
    "* `model_shortcut` will only be used in the first fine-tuning layer, as the later ones will just continue on the same encoder.\n",
    "* Learning rate and its schedule is controlled by `learning_rate`, `lr_schedule`, `warmup_steps`, `max_steps`. For more info, refer to: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f5f39-9d20-4776-970c-2d46464ea983",
   "metadata": {},
   "source": [
    "#### 2.2.1 Use pre-trained models\n",
    "\n",
    "There are two ways to provide pre-trained Transformer encoder:\n",
    "* **Download from huggingface repo** (https://huggingface.co/models): model name provided by `model_shortcut`. (e.x. `bert-base-uncased` or `w11wo/javanese-distilbert-small`)\n",
    "* **Load your custom model from local disk**: model path provided by `init_model_dir`. Model should be loadable through `TransformerMatcher.load()`\n",
    "\n",
    "Note that both `model_shortcut` and `init_model_dir` will only be used in the first fine-tuning layer, as the later ones will just continue on the final state from parent encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0330c-c960-4cdc-bc43-30d0bf256206",
   "metadata": {},
   "source": [
    "A simple example if you want to construct your custom pre-trained model for XR-Transformer fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9570261c-10b4-4c74-b018-0bd3c8b524d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at work_dir/my_pre_trained_model/text_encoder were not used when initializing DistilBertForXMC: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/05/2022 01:43:29 - WARNING - pecos.xmc.xtransformer.matcher - XMC text_model of DistilBertForXMC not initialized from pre-trained model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pecos.xmc.xtransformer.matcher.TransformerMatcher'> model loaded with encoder_type=distilbert num_labels=2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.sparse as smat\n",
    "from pecos.xmc.xtransformer.matcher import TransformerMatcher\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "init_model_dir = \"work_dir/my_pre_trained_model\"\n",
    "os.makedirs(\"work_dir\", exist_ok=True)\n",
    "\n",
    "# example to use your own pre-trained model, here we use huggingface model as an example\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "my_encoder = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# do my own modification/tuning/etc\n",
    "# ...\n",
    "\n",
    "# save my own model to disk\n",
    "my_tokenizer.save_pretrained(f\"{init_model_dir}/text_tokenizer\")\n",
    "my_encoder.save_pretrained(f\"{init_model_dir}/text_encoder\")\n",
    "\n",
    "# then the `work_dir` can be fed as `init_model_dir` as initial model.\n",
    "# Sanity check: if this dir can be loaded via TransformerMatcher.load(*)\n",
    "matcher = TransformerMatcher.load(init_model_dir)\n",
    "print(f\"{matcher.__class__} model loaded with encoder_type={matcher.model_type} num_labels={matcher.nr_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dcefe-9839-46bb-806a-cc83932dadfb",
   "metadata": {},
   "source": [
    "Or you could download our released encoders via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7561e709-f14c-47c4-b14b-d80c62d156bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATASET=\"wiki10-31k\"\n",
    "wget -q https://archive.org/download/xr-transformer-encoders/${DATASET}.tar.gz -O ${DATASET}_encoder.tar.gz\n",
    "mkdir -p ./work_dir/xr-transformer-encoder\n",
    "tar -zxf ./${DATASET}_encoder.tar.gz -C ./work_dir/xr-transformer-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e04d9f9-0e1c-4000-a947-a417d04e84b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pecos.xmc.xtransformer.matcher.TransformerMatcher'> model loaded with encoder_type=bert num_labels=30938\n"
     ]
    }
   ],
   "source": [
    "matcher = TransformerMatcher.load(\"./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder\")\n",
    "print(f\"{matcher.__class__} model loaded with encoder_type={matcher.model_type} num_labels={matcher.nr_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f374aaf-c2cd-4daf-937b-f5974b0d4a75",
   "metadata": {},
   "source": [
    "#### 2.2.2 Bootstrapping and Cost Sensitive Leanring\n",
    "\n",
    "We provide three options to boostrap the XMC head at child level (i.e. $W^{(t+1)}$) from parent level (i.e. $W^{(t)}$):\n",
    "* `bootstrap_method=None`: No bootstrap, $W^{(t+1)}$ will be randomly initialized.\n",
    "* `bootstrap_method='inherit'`: Bootstrap by inherit the weight vector from parent node. \n",
    "* `bootstrap_method='linear'`(default): linear model will be trained on final embeddings of parent layer and be used as initial point for $W^{(t+1)}$.\n",
    "\n",
    "In most cases the default linear bootstrapper would give good enough initial point the XMC heads.\n",
    "Compared with linear bootstrapper, the inherit bootstrapper has less memory/time overhead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8776a-6e46-488c-97cd-d72a5842078b",
   "metadata": {},
   "source": [
    "XR-Transformer allows taking magnutute of label strength into consideration via cost-sensitive learning.\n",
    "This is available even when input label matrix is binary. In this case, the cost (at non-leaf level) will be inferred via label aggregation.\n",
    "\n",
    "To use cost-sensitive fine-tuning, use the `weighted-` version of loss functions. I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6217ae9b-eb10-4dd7-a41d-3b36024ea915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weighted-hinge', 'weighted-squared-hinge']\n"
     ]
    }
   ],
   "source": [
    "print([lf for lf in TransformerMatcher.LOSS_FUNCTION_TYPES.keys() if 'weighted-' in lf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01e03b-427d-47c9-ba5d-8ae45e647996",
   "metadata": {},
   "source": [
    "### 2.3 Linear models with concatenated feature\n",
    "\n",
    "The training of linear models is controlled by the `ranker_params`, which is of the same format as PECOS XR-Linear.\n",
    "\n",
    "User should pay special attention to the `threshold` which controls the sparsification of final linear models.\n",
    "Unlike purely sparse features, the linear models trained on sparse+dense concatenated features are more sensitive to the sparsification.\n",
    "Usually `threshold=0.01` or `0.001` is recommended for XR-Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8531b6d-f96f-476e-836e-01a2f6daa35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForXMC: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForXMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForXMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 85.22 82.55 77.26 72.15 67.42 63.13 59.33 56.08 53.02 50.24\n",
      "recall = 5.05 9.76 13.58 16.74 19.41 21.68 23.64 25.41 26.92 28.22\n"
     ]
    }
   ],
   "source": [
    "from pecos.xmc.xtransformer.module import MLProblemWithText\n",
    "prob = MLProblemWithText(X_txt, Y, X_feat=X_feat)\n",
    "\n",
    "# disable fine-tuning, use pre-trained bert model from huggingface\n",
    "wiki31k_train_params.do_fine_tune = False\n",
    "\n",
    "# this will be slow on CPU only machine\n",
    "xtf_pretrained = XTransformer.train(\n",
    "    prob,\n",
    "    clustering=preliminary_hlt,\n",
    "    train_params=wiki31k_train_params,\n",
    "    pred_params=wiki31k_pred_params,\n",
    ")\n",
    "\n",
    "X_feat_tst = smat_util.load_matrix(\"xmc-base/wiki10-31k/tfidf-attnxml/X.tst.npz\", dtype=np.float32)\n",
    "Y_tst = smat_util.load_matrix(\"xmc-base/wiki10-31k/Y.tst.npz\", dtype=np.float32)\n",
    "\n",
    "with open(\"xmc-base/wiki10-31k/X.tst.txt\", 'r') as fin:\n",
    "    X_txt_tst = [xx.strip() for xx in fin.readlines()]\n",
    "\n",
    "P_pretrained = xtf_pretrained.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_pretrained, topk=10)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ecca84-75a5-4d38-b1ad-3256761fb695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 87.95 83.54 78.79 73.95 69.43 65.14 61.08 57.70 54.63 51.97\n",
      "recall = 5.25 9.89 13.84 17.14 19.99 22.36 24.35 26.16 27.73 29.21\n"
     ]
    }
   ],
   "source": [
    "# use fine-tuned bert model\n",
    "wiki31k_train_params.matcher_params_chain[0].init_model_dir = \"./work_dir/xr-transformer-encoder/wiki10-31k/bert/text_encoder\"\n",
    "\n",
    "# this will be slow on CPU only machine\n",
    "xtf_fine_tuned = XTransformer.train(\n",
    "    prob,\n",
    "    clustering=preliminary_hlt,\n",
    "    train_params=wiki31k_train_params,\n",
    "    pred_params=wiki31k_pred_params,\n",
    ")\n",
    "\n",
    "P_fine_tuned = xtf_fine_tuned.predict(X_txt_tst, X_feat=X_feat_tst)\n",
    "metrics = smat_util.Metrics.generate(Y_tst, P_fine_tuned, topk=10)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c3e79-97bb-4cfa-968b-5a018c5ef2f9",
   "metadata": {},
   "source": [
    "# (BETA) Distributed PECOS\n",
    "\n",
    "`pecos.distributed` is a PECOS module that enables distributed training.\n",
    "\n",
    "Currently the following sub-modules are implemented:\n",
    "\n",
    "* Distributed X-Linear ([`pecos.distributed.xmc.xlinear`](xmc/xlinear/README.md))\n",
    "\n",
    "We are working to implement more distributed algorithms for PECOS existing models, please watch out for our newest releases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65b077-2339-48b6-92e9-554d676a2b6b",
   "metadata": {},
   "source": [
    "## 1. Distributed XR-Linear\n",
    "\n",
    "`pecos.distributed.xmc.xlinear` enables distributed training for PECOS XLinear model (`pecos.xmc.xlinear`).\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* **Hardware**: \n",
    "    * Cluster of machines connected by network which can password-less SSH to each other.\n",
    "      * IP address of every machine in the cluster is known.\n",
    "    * Shared network disk mounted on all machines.\n",
    "      * For accessing data and saving trained models.\n",
    "\n",
    "* **Software**: Install the following software on **every** machine of your cluster\n",
    "    * MPI and mpi4py\n",
    "    \n",
    "Due to the hardware constraint during the tutorial, we only include a basic example in local mode here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2834997c-4bd9-4732-acb6-f7e4e7f6a090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using built-in specs.\n",
      "COLLECT_GCC=/usr/bin/gcc\n",
      "COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/7/lto-wrapper\n",
      "Target: x86_64-redhat-linux\n",
      "Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,objc,obj-c++,fortran,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-libmpx --enable-libsanitizer --enable-gnu-indirect-function --enable-libcilkrts --enable-libatomic --enable-libquadmath --enable-libitm --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\n",
      "Thread model: posix\n",
      "gcc version 7.3.1 20180712 (Red Hat 7.3.1-15) (GCC) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/amazon/openmpi/bin/mpiexec\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: mpi4py in /home/ec2-user/repo/tutorial-env/lib/python3.9/site-packages (3.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/repo/tutorial-env/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! I am process 0 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 1 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 2 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 3 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 4 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 5 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 6 of 8 on ip-[MASKED].ec2.internal.\n",
      "Hello, World! I am process 7 of 8 on ip-[MASKED].ec2.internal.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# check the required dependencies\n",
    "mpicc -v\n",
    "which mpiexec\n",
    "python3 -m pip install mpi4py\n",
    "mpiexec -n 8 python3 -m mpi4py.bench helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04cd3aa-3d7a-4e11-8b15-5074d24525ec",
   "metadata": {},
   "source": [
    "### Basic Usage\n",
    "\n",
    "Below is a simple showcase of the usage of `pecos.distributed.xmc.xlinear.train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b7e689-480d-4c81-a408-d4b840695880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/05/2022 01:53:16 - INFO - pecos.utils.profile_util - psutil module installed, will print memory info.\n",
      "08/05/2022 01:53:16 - INFO - pecos.utils.profile_util - psutil module installed, will print memory info.\n",
      "08/05/2022 01:53:16 - INFO - __main__ - Started loading data on Rank 0 ... RSS 89.9 MB. Full mem info: pmem(rss=94277632, vms=805863424, shared=45461504, text=2732032, lib=0, data=156913664, dirty=0)\n",
      "08/05/2022 01:53:16 - INFO - __main__ - Started loading data on Rank 1 ... RSS 89.7 MB. Full mem info: pmem(rss=94011392, vms=805867520, shared=45207552, text=2732032, lib=0, data=156917760, dirty=0)\n",
      "08/05/2022 01:53:16 - INFO - __main__ - Done loading data on Rank 1. RSS 166.4 MB. Full mem info: pmem(rss=174432256, vms=884928512, shared=45535232, text=2732032, lib=0, data=235978752, dirty=0)\n",
      "08/05/2022 01:53:16 - INFO - __main__ - Done loading data on Rank 0. RSS 166.5 MB. Full mem info: pmem(rss=174637056, vms=884924416, shared=45735936, text=2732032, lib=0, data=235974656, dirty=0)\n",
      "08/05/2022 01:53:16 - INFO - pecos.distributed.xmc.base - Starts creating label embedding PIFA for meta tree on Rank 0 node... RSS 166.5 MB. Full mem info: pmem(rss=174637056, vms=884924416, shared=45735936, text=2732032, lib=0, data=235974656, dirty=0)\n",
      "08/05/2022 01:53:19 - INFO - pecos.distributed.xmc.base - Done creating label embedding PIFA for meta tree on Rank 0 node. RSS 1005.2 MB. Full mem info: pmem(rss=1054044160, vms=1838071808, shared=46354432, text=2732032, lib=0, data=1123237888, dirty=0)\n",
      "08/05/2022 01:53:19 - INFO - pecos.distributed.xmc.base - Starts generating meta tree cluster on main node...\n",
      "08/05/2022 01:53:19 - INFO - pecos.distributed.xmc.base - Determined meta-tree leaf clusters number: 2. 2 nodes will train 2 sub-trees. Number of data labels: 30938, nr_splits: 16\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - Done generating meta tree cluster. RSS 1005.7 MB. Full mem info: pmem(rss=1054523392, vms=1838071808, shared=46813184, text=2732032, lib=0, data=1123237888, dirty=0)\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - Rank 0 get 1 sub-tree assignments.\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - On rank 0, 0th sub-tree assignment has 15469 labels: [2, 4, 6, 8, 9, 10, 12, 14, 15, 18]...\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - Rank 1 get 1 sub-tree assignments.\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - On rank 1, 0th sub-tree assignment has 15469 labels: [0, 1, 3, 5, 7, 11, 13, 16, 17, 19]...\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - Starts creating label embedding PIFA for 0th sub-tree on rank 0... RSS 170.4 MB. Full mem info: pmem(rss=178642944, vms=962002944, shared=47120384, text=2732032, lib=0, data=247169024, dirty=0)\n",
      "08/05/2022 01:53:23 - INFO - pecos.distributed.xmc.base - Starts creating label embedding PIFA for 0th sub-tree on rank 1... RSS 166.8 MB. Full mem info: pmem(rss=174923776, vms=885452800, shared=45776896, text=2732032, lib=0, data=236503040, dirty=0)\n",
      "08/05/2022 01:53:25 - INFO - pecos.distributed.xmc.base - Done creating label embedding PIFA for 0th sub-tree on rank 1. RSS 473.8 MB. Full mem info: pmem(rss=496820224, vms=1280671744, shared=46534656, text=2732032, lib=0, data=565837824, dirty=0)\n",
      "08/05/2022 01:53:25 - INFO - pecos.distributed.xmc.base - Starts generating 0th sub-tree cluster on rank 1...\n",
      "08/05/2022 01:53:26 - INFO - pecos.distributed.xmc.base - Done creating label embedding PIFA for 0th sub-tree on rank 0. RSS 706.0 MB. Full mem info: pmem(rss=740335616, vms=1523580928, shared=47185920, text=2732032, lib=0, data=808747008, dirty=0)\n",
      "08/05/2022 01:53:26 - INFO - pecos.distributed.xmc.base - Starts generating 0th sub-tree cluster on rank 0...\n",
      "08/05/2022 01:53:31 - INFO - pecos.distributed.xmc.base - Done generating 0th sub-tree cluster on rank 1. RSS 474.4 MB. Full mem info: pmem(rss=497467392, vms=1280671744, shared=47153152, text=2732032, lib=0, data=565837824, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - Done generating 0th sub-tree cluster on rank 0. RSS 706.1 MB. Full mem info: pmem(rss=740356096, vms=1523580928, shared=47194112, text=2732032, lib=0, data=808747008, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - Starts assmebling cluster chain... RSS 172.2 MB. Full mem info: pmem(rss=180523008, vms=963616768, shared=47194112, text=2732032, lib=0, data=248782848, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - Done assmebling cluster chain. Split depth: 1. Chain length: 4 RSS 172.2 MB. Full mem info: pmem(rss=180523008, vms=963616768, shared=47194112, text=2732032, lib=0, data=248782848, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - Broadcasting distributed cluster chain from Node 0...\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - Done broadcast distributed cluster chain from Node 0.\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Starts receiving sub-training jobs from source 0 for rank 1...\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - meta, sub negative samples: 32 76\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - meta_tree_leaf_cluster: (30938, 32)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Main node workload: 216470.70175438595\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Min worker node workload, machine rank: (204720, 0). Max worker node workload, machine rank: (204720, 0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Training jobs for all Sub-trees divided onto 2 machines: Main node will train for 7 sub-trees, Worker nodes will train for [25] sub-trees, worker receive order: [1].\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Starts sending sub-training jobs from node 0 to 1...\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Done sending sub-training jobs from node 0 to 1.\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Rank 0 starts meta-tree training... RSS 173.0 MB. Full mem info: pmem(rss=181362688, vms=964771840, shared=47194112, text=2732032, lib=0, data=249937920, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Done receiving sub-training jobs from source 0 for rank 1.\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Rank 1 get 25 sub-trees to train\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.xlinear.model - Rank 1 starts sub-tree training... RSS 170.2 MB. Full mem info: pmem(rss=178483200, vms=961769472, shared=47218688, text=2732032, lib=0, data=246935552, dirty=0)\n",
      "08/05/2022 01:53:37 - INFO - pecos.distributed.xmc.base - meta_tree_leaf_cluster: (30938, 32)\n",
      "08/05/2022 01:53:52 - INFO - pecos.distributed.xmc.xlinear.model - Rank 0 done meta-tree training. RSS 195.2 MB. Full mem info: pmem(rss=204632064, vms=983449600, shared=47259648, text=2732032, lib=0, data=273154048, dirty=0)\n",
      "08/05/2022 01:53:52 - INFO - pecos.distributed.xmc.xlinear.model - Rank 0 get 7 sub-trees to train\n",
      "08/05/2022 01:53:52 - INFO - pecos.distributed.xmc.xlinear.model - Rank 0 starts sub-tree training... RSS 195.2 MB. Full mem info: pmem(rss=204632064, vms=983449600, shared=47259648, text=2732032, lib=0, data=273154048, dirty=0)\n",
      "08/05/2022 01:54:37 - INFO - pecos.distributed.xmc.xlinear.model - Rank 0 total 7 sub-tree training finished. RSS 239.9 MB. Full mem info: pmem(rss=251518976, vms=1030242304, shared=47325184, text=2732032, lib=0, data=319946752, dirty=0)\n",
      "08/05/2022 01:54:37 - INFO - pecos.distributed.xmc.xlinear.model - Main node start recv 25 sub-tree models from rank 1\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Rank 1 total 25 sub-tree training finished. RSS 251.5 MB. Full mem info: pmem(rss=263733248, vms=1044664320, shared=47349760, text=2732032, lib=0, data=334344192, dirty=0)\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Rank 1 node starts sending 25 sub-tree models.\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Main node done receive 25 sub-tree models from rank 1\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Rank 1 node done sending 25 sub-tree models.\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Reconstruct full model on Rank 0 node... RSS 240.2 MB. Full mem info: pmem(rss=251850752, vms=1030242304, shared=47398912, text=2732032, lib=0, data=319946752, dirty=0)\n",
      "08/05/2022 01:55:42 - INFO - pecos.distributed.xmc.xlinear.model - Done reconstruct full model on Rank 0 node. RSS 315.7 MB. Full mem info: pmem(rss=331001856, vms=1109413888, shared=47398912, text=2732032, lib=0, data=399118336, dirty=0)\n",
      "08/05/2022 01:55:42 - INFO - __main__ - Saving model to work_dir/dist_xlinear_model...\n",
      "08/05/2022 01:55:42 - INFO - __main__ - Done saving model.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mpiexec -n 2 \\\n",
    "python3 -m pecos.distributed.xmc.xlinear.train \\\n",
    "-x xmc-base/wiki10-31k/tfidf-attnxml/X.trn.npz \\\n",
    "-y xmc-base/wiki10-31k/Y.trn.npz \\\n",
    "-m work_dir/dist_xlinear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb08655-7de1-426c-b203-0c012ac50c7a",
   "metadata": {},
   "source": [
    "We didn't setup the multi-node cluster therefore only single machine is used here. In practice, you can store your machines' addresses in `hostfile` and run the distributed training via \n",
    "```\n",
    "mpiexec -f hostfile -n ${NUM_MACHINE} python3 -m pecos.distributed.xmc.xlinear.train [..]\n",
    "```\n",
    "\n",
    "The distributed trained model is serialized in the same way as the single node trained model. We can use the same way to predict and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39cbf96-6cf2-4721-86d7-90cc27bdbe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== evaluation results ====\n",
      "prec   = 84.05 78.20 72.57 67.90 63.90 60.17 56.86 53.87 51.31 48.82\n",
      "recall = 4.97 9.17 12.63 15.62 18.26 20.52 22.48 24.25 25.88 27.25\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pecos.xmc.xlinear.predict \\\n",
    "-x xmc-base/wiki10-31k/tfidf-attnxml/X.tst.npz \\\n",
    "-y xmc-base/wiki10-31k/Y.tst.npz \\\n",
    "-m work_dir/dist_xlinear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14fb96-7ab3-4e1c-8b23-f1ef76d83a0d",
   "metadata": {},
   "source": [
    "## Distributed Training Algorithm\n",
    "\n",
    "Because of the model separability of PECOS XR-Linear model, we can split the original problem into multiple independent problems:\n",
    "* **One meta-problem**: XMC problem to match an input X to K clusters\n",
    "* **K sub-problems**: XMC problem to rank the labels in one of the K clusters for an input X.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/dist-xlinear.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522bf81-3d13-4821-815e-ddb24cb412d0",
   "metadata": {},
   "source": [
    "In addition to distributed training, `pecos.distributed.xmc.xlinear` also has the following features:\n",
    "\n",
    "* **Distributed Hierarchical Clustering**: We leverage the same meta-sub problem split to build the Hierarchical label tree. Since that building label feature for a huge dataset could be memory intensive for meta node, we provide option to use simpler label embedding for meta-tree generation:`--meta-label-embedding-method pii`\n",
    "* **Load Balancing**: Beacuse of the long tail distribution in most XMC problems, workload to train each sub-problem varies a lot. To address that, the distributed training algorithm does load balancing when K > #workers. The sub-tree number K can be controlled via `--min-n-sub-tree`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
